{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba941b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()  \n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from langdetect import detect\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "#ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2da579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading and preprocessing dataset...\n",
      "INFO:__main__:Successfully loaded and renamed columns\n",
      "INFO:__main__:Cleaning and processing texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e3c918c8384049b79773c6a62ddabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bff9cd9c4a94f96b09b7fa56a3a8d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e90fa96bc9d4f55b41caf5564cd6cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Checking for non-Arabic texts...\n",
      "INFO:__main__:Final dataset size after cleaning: 15103 rows\n",
      "INFO:__main__:\n",
      "Text length distribution after processing:\n",
      "INFO:__main__:count    15103.000000\n",
      "mean      1344.492220\n",
      "std       1045.321623\n",
      "min         54.000000\n",
      "25%        894.000000\n",
      "50%       1246.000000\n",
      "75%       1596.500000\n",
      "max      18954.000000\n",
      "Name: text_length, dtype: float64\n",
      "INFO:__main__:\n",
      "Summary ratio distribution:\n",
      "INFO:__main__:count    15103.000000\n",
      "mean         0.124586\n",
      "std          0.058973\n",
      "min          0.050000\n",
      "25%          0.077683\n",
      "50%          0.109297\n",
      "75%          0.157426\n",
      "max          0.300000\n",
      "Name: summary_ratio, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Function to clean text by removing special characters and extra spaces\n",
    "def clean_text(text):\n",
    "    # Handle empty or NaN values\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and clean\n",
    "    text = str(text)\n",
    "    # Keep only Arabic characters and spaces\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text)\n",
    "    # Remove extra spaces by splitting and joining with single space\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "# Function to process long texts by keeping important sentences\n",
    "# This helps reduce the input size while maintaining key information\n",
    "def process_text_length(text, max_length=2000):\n",
    "    # If text is too long, keep only important parts\n",
    "    if len(text) > max_length:\n",
    "        # Split text into sentences using period as delimiter\n",
    "        sentences = text.split('.')\n",
    "        # Keep first sentence (main topic)\n",
    "        processed_text = sentences[0] + '.'\n",
    "        # Keep middle sentence - often contains supporting details\n",
    "        mid_idx = len(sentences) // 2\n",
    "        processed_text += sentences[mid_idx] + '.'\n",
    "        # Keep last sentence (conclusion) - usually summarizes the main points\n",
    "        processed_text += sentences[-1] + '.'\n",
    "        return processed_text.strip()\n",
    "    return text\n",
    "\n",
    "# Function to check if text is in Arabic\n",
    "# Uses langdetect library to identify the language\n",
    "def is_arabic(text):\n",
    "    try:\n",
    "        return detect(text) == 'ar'  # 'ar' is the language code for Arabic\n",
    "    except:\n",
    "        return False  # Return False if language detection fails\n",
    "\n",
    "# Main data loading and preprocessing\n",
    "logger.info(\"Loading and preprocessing dataset...\")\n",
    "try:\n",
    "    # Load the dataset\n",
    "    df = pd.read_excel(\"Text summarization dataset.xlsx\")\n",
    "    \n",
    "    # Fix column names and remove header\n",
    "    df = df.iloc[1:].reset_index(drop=True)\n",
    "    df.columns = ['summary', 'text']  # Rename columns for clarity\n",
    "    logger.info(\"Successfully loaded and renamed columns\")\n",
    "    \n",
    "    # Clean the data\n",
    "    logger.info(\"Cleaning and processing texts...\")\n",
    "    df['text'] = df['text'].progress_apply(clean_text)\n",
    "    df['summary'] = df['summary'].progress_apply(clean_text)\n",
    "    df['text'] = df['text'].progress_apply(process_text_length)\n",
    "    \n",
    "    # Remove bad data\n",
    "    df = df.dropna()  # Remove missing values\n",
    "    df = df.drop_duplicates()  # Remove duplicates\n",
    "    df = df[df['text'].str.len() >= 50]  # Remove very short texts\n",
    "    \n",
    "    # Calculate text statistics\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['summary_length'] = df['summary'].str.len()\n",
    "    df['summary_ratio'] = df['summary_length'] / df['text_length']\n",
    "    \n",
    "    # Keep only reasonable summary lengths\n",
    "    df = df[df['summary_ratio'].between(0.05, 0.3)]\n",
    "    \n",
    "    # Remove non-Arabic texts\n",
    "    logger.info(\"Checking for non-Arabic texts...\")\n",
    "    df = df[df['text'].apply(is_arabic)]\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    logger.info(f\"Final dataset size after cleaning: {len(df)} rows\")\n",
    "    logger.info(\"\\nText length distribution after processing:\")\n",
    "    logger.info(df['text_length'].describe())\n",
    "    logger.info(\"\\nSummary ratio distribution:\")\n",
    "    logger.info(df['summary_ratio'].describe())\n",
    "    \n",
    "    # Split data for training\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Convert pandas DataFrames to HuggingFace Dataset format\n",
    "    # This format is required for the transformer model\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Error: Dataset file 'Text summarization dataset.xlsx' not found.\")\n",
    "    logger.error(\"Please make sure the dataset file is in the same directory as the script.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred: {str(e)}\")\n",
    "    logger.error(\"Please check the dataset format and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5ca5f",
   "metadata": {},
   "source": [
    "### The compute_metrics, I never actually used. My hardware couldn't handel it. in the training arg. I seted the evalution_strategy to no. When you want to use it, do these changes: \n",
    "evaluation_strategy from \"no\" to \"epoch\", Enables evaluation at the end of each epoch\n",
    "\n",
    "metric_for_best_model from \"loss\" to \"rouge1\", Uses ROUGE-1 score to determine the best model\n",
    " \n",
    " greater_is_better from False to True, Indicates that higher ROUGE scores are better \n",
    "\n",
    " load_best_model_at_end from False to True, Saves the model checkpoint with the best ROUGE score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a777d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111e2ee08f674f938f41ead8c233def4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13592 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6239cf2a74b74de3ba344230e6d44902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1511 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "INFO:root:Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d377e876127e444baa1934bc8d192b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1060 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BarthezTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.4462, 'learning_rate': 9.433962264150943e-07, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3176, 'learning_rate': 5.89098532494759e-05, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7216, 'learning_rate': 6.49895178197065e-06, 'epoch': 4.71}\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: './results\\\\tmp-checkpoint-1060' -> './results\\\\checkpoint-1060'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Start the training process\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# This will take some time depending on your hardware\u001b[39;00m\n\u001b[0;32m    125\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Save the trained model for later use\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# This allows us to use the model without retraining\u001b[39;00m\n\u001b[0;32m    130\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./arabic_summarization_model/final_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1538\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1539\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1540\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1541\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1542\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   1933\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m   2280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\Azooo\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2395\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2391\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmain_process_first(\n\u001b[0;32m   2392\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRenaming model checkpoint folder to true location\u001b[39m\u001b[38;5;124m\"\u001b[39m, local\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_on_each_node\n\u001b[0;32m   2393\u001b[0m     ):\n\u001b[0;32m   2394\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(staging_output_dir):\n\u001b[1;32m-> 2395\u001b[0m             os\u001b[38;5;241m.\u001b[39mrename(staging_output_dir, output_dir)\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;66;03m# Maybe delete some older checkpoints.\u001b[39;00m\n\u001b[0;32m   2398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: './results\\\\tmp-checkpoint-1060' -> './results\\\\checkpoint-1060'"
     ]
    }
   ],
   "source": [
    "# Load the AraBART model and tokenizer for Arabic text summarization\n",
    "# Using moussaKam's pre-trained AraBART model which is specifically designed for Arabic text\n",
    "model_name = \"moussaKam/AraBART\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Check if GPU is available to speed up training\n",
    "# This is important because training on GPU can be significantly faster\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to prepare our data for the model\n",
    "# This function handles tokenization of both input text and target summaries\n",
    "def preprocess_function(examples):\n",
    "    # Convert input text to tokens\n",
    "    inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=512,  # Limit input length to 512 tokens\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Convert target summaries to tokens\n",
    "    # Using max_length=128 for summaries as they should be shorter than input\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"],\n",
    "            max_length=128,  # Limit summary length to 128 tokens\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "# Create datasets from our pandas DataFrames\n",
    "# This converts our data into a format that HuggingFace can work with\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Tokenize our datasets using the preprocessing function\n",
    "# This applies our tokenization to all examples in the dataset\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_eval = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "# Function to calculate ROUGE scores for evaluation\n",
    "# ROUGE is a metric that measures the quality of summaries\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Convert predictions back to text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Fix labels for decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate ROUGE scores to measure summary quality\n",
    "    # We use ROUGE  for comprehensive evaluation\n",
    "    rouge_scores = rouge.get_scores(decoded_preds, decoded_labels, avg=True)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': rouge_scores['rouge-1']['f'],\n",
    "        'rouge2': rouge_scores['rouge-2']['f'],\n",
    "        'rougeL': rouge_scores['rouge-l']['f']\n",
    "    }\n",
    "\n",
    "# Set up training configuration\n",
    "# These parameters are carefully chosen to balance training speed and model performance\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=4,  # Small batch size due to memory constraints\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    evaluation_strategy=\"no\",  # Skip evaluation during training to save time\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=False,  # Don't load best model since we're not evaluating\n",
    "    logging_dir='./logs',\n",
    "    logging_first_step=True,\n",
    "    disable_tqdm=False,\n",
    "    fp16=True if torch.cuda.is_available() else False,  # Use mixed precision if GPU available\n",
    "    gradient_accumulation_steps=16,  # Accumulate gradients to simulate larger batch size\n",
    "    gradient_checkpointing=True,  # Save memory by recomputing activations\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=1.0,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "# Initialize the trainer with our model and data\n",
    "# This sets up everything needed for training\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "# This will take some time depending on your hardware\n",
    "logging.info(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model for later use\n",
    "# This allows us to use the model without retraining\n",
    "trainer.save_model(\"./summarization_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
