{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df75be8",
   "metadata": {},
   "source": [
    "# CS365 Project Phase 2: Arabic NLP Classification\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### Project Overview and Objectives\n",
    "This project implements both traditional and modern approaches for Arabic text classification using the KalimatCorpus-2.0 dataset. The main objectives are:\n",
    "- Implement traditional machine learning approaches (SVM, Naive Bayes with TF-IDF)\n",
    "- Implement modern deep learning approaches (BiLSTM, AraBERT)\n",
    "- Compare performance across different methods\n",
    "- Analyze trade-offs between traditional and modern approaches\n",
    "\n",
    "### Dataset Description\n",
    "The chosen dataset is KalimatCorpus-2.0, which contains Arabic news articles from multiple categories:\n",
    "- **Culture**: Cultural news and articles\n",
    "- **Economy**: Economic and business news\n",
    "- **International**: International news coverage\n",
    "- **Local**: Local Omani news\n",
    "- **Religion**: Religious content and discussions\n",
    "- **Sports**: Sports news and coverage\n",
    "\n",
    "### Background on Arabic NLP Challenges\n",
    "Arabic NLP presents unique challenges including:\n",
    "- Right-to-left script direction\n",
    "- Complex morphology and root-based word formation\n",
    "- Diacritics and various letter forms\n",
    "- Dialectal variations and Modern Standard Arabic differences\n",
    "- Limited preprocessing tools compared to English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7163b8e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Data Exploration and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099170f",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Downloading each article category in KalimatCorpus-2.0 from sourceforge.net\n",
    "- Each category is in a folder, with all articles as .txt\n",
    "- All articles are written words each in a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1681a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing.dummy as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe1b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat_base = \"data/KalimatCorpus-2.0\"\n",
    "expected_dirs = os.listdir(kalimat_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcc2fcc",
   "metadata": {},
   "source": [
    "#### Checking for the existence of the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_kalimat_structure_os():\n",
    "    missing = [d for d in expected_dirs if not os.path.isdir(os.path.join(kalimat_base, d))]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"âŒ Missing folders: {missing}\")\n",
    "    else:\n",
    "        count = 0\n",
    "        for d in expected_dirs:\n",
    "            folder_path = os.path.join(kalimat_base, d)\n",
    "            count += len([f for f in os.listdir(folder_path)])\n",
    "        print(f\"âœ… Kalimat Corpus is ready with {count} .txt files\")\n",
    "\n",
    "check_kalimat_structure_os()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292facbf",
   "metadata": {},
   "source": [
    "#### `load_kalimat_articles`\n",
    "We went through each folder `category` pass it to `load_kalimat_articles`. \n",
    "This function will read all the articles, append them to articles list with the following information:\n",
    "- `category`: the name of the folder\n",
    "- `filename`: the name of the file\n",
    "- `text`: the content of the article\n",
    "- `text_length`: the length of the article in characters\n",
    "- `word_count`: the number of words in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0004e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kalimat_articles(category):\n",
    "    category_path = os.path.join(kalimat_base, category)\n",
    "    if not os.path.isdir(category_path):\n",
    "        print(f\"âŒ Category '{category}' does not exist in the Kalimat Corpus.\")\n",
    "        return []\n",
    "\n",
    "    articles = []\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(category_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                words = [line.strip() for line in f if line.strip()]\n",
    "                text = \" \".join(words)\n",
    "                articles.append({\n",
    "                    \"category\": category.replace(\"articles\", \"\").upper(),\n",
    "                    \"filename\": filename,\n",
    "                    \"text\": text,\n",
    "                    \"text_length\": len(text),\n",
    "                    \"word_count\": len(words)\n",
    "                })\n",
    "\n",
    "    print(f\"âœ… Loaded {len(articles)} articles from category '{category}'\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1168a9",
   "metadata": {},
   "source": [
    "Parellalize the loading of articles using `multiprocessing` to speed up the process.\n",
    "\n",
    "Then in `dataset` we flat the list of articles to one array instead of a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901e61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_articles_parallel():\n",
    "    with mp.Pool(processes=min(len(expected_dirs), int(os.cpu_count() / 2))) as pool:\n",
    "        results = pool.map(load_kalimat_articles, expected_dirs)\n",
    "    \n",
    "    dataset = [article for category_articles in results for article in category_articles]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ac0b8",
   "metadata": {},
   "source": [
    "`if __name__ == \"__main__\"` is Important to solve an issue in Windows OS with `multiprocessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = load_all_articles_parallel()\n",
    "    print(f\"âœ… Dataset loaded with {len(dataset)} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4cb2b",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Say Hello to pandas!\n",
    "\n",
    "We will start by constructing a DataFrame from the dataset list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc09f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdataset\u001b[49m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(), df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(), df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c39c5",
   "metadata": {},
   "source": [
    "#### Setup necessary libraries\n",
    "- `nltk` for text processing\n",
    "- `nltk.corpus.stopwords` for stop words\n",
    "- `regex`\n",
    "\n",
    "Then downloading the stop words using `nltk.download()` function.\n",
    "- `stopwords` is a list of common words that are not useful for text analysis (e.g. \"ÙÙŠ\", \"Ù…Ù†\", \"Ø¥Ù„Ù‰\", \"Ø¹Ù„Ù‰\", \"Ùˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efafb5bf",
   "metadata": {},
   "source": [
    "Adding additional stopwords based on our analysis of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "stemmer = nltk.stem.ISRIStemmer() # Worsen the results\n",
    "for word in ['ÙÙŠ', 'Ø§Ù†', 'Ø§Ù†', 'Ø§Ù„Ù‰', 'Ø§Ùˆ', 'ÙÙ‰']: arabic_stopwords.add(word)\n",
    "print(f\"Stop words count: {len(arabic_stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee8481",
   "metadata": {},
   "source": [
    "#### `preprocess_text`\n",
    "We applied `preprocess_text` to clean and normalize the Arabic text before modeling.  \n",
    "This function performs the following steps:\n",
    "- **Remove** punctuation, digits (Arabic and English), and English letters.\n",
    "- **Normalize** Arabic letters by unifying variants (e.g., \"Ø£\", \"Ø¥\", \"Ø¢\" â†’ \"Ø§\").\n",
    "- **Remove** Arabic diacritics and extra whitespace.\n",
    "- **Tokenize** the text and **remove** Arabic stopwords.\n",
    "- Finally, **join** the tokens back into a cleaned string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    text = re.sub(r'\\p{P}+|\\$', '', text)  # remove all punctuation (English + Arabic)\n",
    "    text = re.sub(r'[0-9Ù -Ù©]', '', text)  # remove Arabic and English digits\n",
    "    text = re.sub(r'[a-zA-Z]', '', text)  # remove English letters\n",
    "    text = re.sub(r'[Ø§Ø¢Ø¥Ø£]', 'Ø§', text)  # replace Arabic letter with hamza with 'Ø§'\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)  # remove Arabic diacritics\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # clean extra spaces\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in arabic_stopwords]\n",
    "\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocess_text(\"!Ù…Ø±Ø­Ø¨Ø§Ù‹... Ù‡Ø°Ø§ Ù†ÙŽØµÙ‘ÙŒ ØªÙŽØ¬Ù’Ø±ÙÙŠØ¨ÙÙŠÙŒ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 123 Ø£Ø±Ù‚Ø§Ù… Ù¤Ù¥Ù¦ØŒ ÙÙŠ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ… @#$%ØŒ ÙƒÙ„Ù…Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© like This.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562dbc4",
   "metadata": {},
   "source": [
    "Now we can apply the `preprocess_text` function to the DataFrame creating a new column `processed_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7641e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ae2b8",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf0802ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from collections import Counter\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "%matplotlib inline\n",
    "\n",
    "fm.fontManager.addfont('arial-unicode-ms.ttf')\n",
    "arabic_font = fm.FontProperties(fname='arial-unicode-ms.ttf')\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = arabic_font.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_duplicate'] = df.duplicated(subset='processed_text', keep=False)\n",
    "\n",
    "# Group and make sure both True/False appear\n",
    "counts = df.groupby(['category', 'is_duplicate']).size().unstack().reindex(columns=[False, True], fill_value=0)\n",
    "counts.columns = ['Unique', 'Duplicate']\n",
    "print(counts)\n",
    "# Plot\n",
    "counts.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='tab10')\n",
    "plt.title(\"Stacked Bar of Unique vs Duplicate Articles per Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Article Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ffac47",
   "metadata": {},
   "source": [
    "#### 1- Stacked Bar: Unique vs Duplicate Articles per Category\n",
    "While most categories have a healthy distribution of unique articles, the `RELIGION` category has a significant number of duplicate articles. This could bias classification models if not properly handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e4f24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7b021",
   "metadata": {},
   "source": [
    "#### 2- Vocabulary Size\n",
    "After preprocessing, the corpus contains ~235k unique words. Such size will impact the dimensionality of the feature extraction methods like TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "df['processed_text'].str.split().apply(vocab.update)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8a2a3",
   "metadata": {},
   "source": [
    "#### 3- Top 20 Most Common Words (Bar Chart)\n",
    "The most frequent words in the corpus. we can see \"Ø§Ù„Ø³Ù„Ø·Ù†Ø©\" which make sense as the corpus if of Omani articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f58b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter()\n",
    "_ = df['processed_text'].str.split().apply(word_counts.update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = word_counts.most_common(20)\n",
    "\n",
    "words, counts = zip(*common_words)\n",
    "display_words = list(map(get_display, map(arabic_reshaper.reshape, words)))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(display_words, counts)\n",
    "plt.title(\"Top 20 Most Common Words in the Corpus\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf74f1c",
   "metadata": {},
   "source": [
    "#### 4- Article Word Counts per Category (Box Plot)\n",
    "The median article length is different across categories. `RELIGION` articles tend to be longer on average, while `SPORTS` articles are shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "df.boxplot(column='word_count', by='category', grid=False, rot=10)\n",
    "plt.title('Article Words Counts per Category') \n",
    "plt.suptitle('')  # Remove the automatic \"Boxplot grouped by\" title\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ef675",
   "metadata": {},
   "source": [
    "#### 5- Top 20 Bigrams (Horizontal Bar Chart)\n",
    "Common bigrams such as \"Ø§Ù„Ø³Ù„Ø·Ø§Ù† Ù‚Ø§Ø¨ÙˆØ³\",\"Ø¨Ù† Ø³Ø¹ÙŠØ¯\", and \"Ù…Ø­Ù…Ø¯ Ø¨Ù†\" appear frequently in the corpus, Capturing common word expressions in Omani journalism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2654bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), max_features=20)\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "bigrams = vectorizer.get_feature_names_out()\n",
    "\n",
    "counts = X.sum(axis=0).A1\n",
    "display_words = list(map(get_display, map(arabic_reshaper.reshape, bigrams)))\n",
    "\n",
    "bigrams_counts = list(zip(display_words, counts))\n",
    "bigrams_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "sorted_display_words, sorted_counts = zip(*bigrams_counts)\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(sorted_display_words, sorted_counts)\n",
    "plt.title('Top 20 Bigrams (Sorted)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfcfbcb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Phase 1: Traditional Approaches\n",
    "\n",
    "### Task 1: Traditional Text Classification\n",
    "Traditional machine learning approaches using feature extraction methods and classical algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7599a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7f3f3",
   "metadata": {},
   "source": [
    "We Tried every combination of Traditional methods `BoW` , `TfIdf` and `SVM` , `Naive Bayes` , `Random Forest` for classification.\n",
    "\n",
    "In general `TfIdf` was better than `BoW` as for the algorithms of learning the best accuracy was from `SVM` with `TfIdf` . `Naive bayes` is the fastest with under 1 sec and very good accuracy. `Random Forest` has good accuracy but it took longer time ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "X = tfidf_matrix\n",
    "y = df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "svm_classifier = LinearSVC(random_state=42, C=1.0)\n",
    "nb_classifier = MultinomialNB(alpha=0.01)\n",
    "svm_classifier.fit(X_train, y_train), nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e4e68",
   "metadata": {},
   "source": [
    "#### Results Summary\n",
    "- `SVM` + `TfIdf` : Gave us the best f1-score average accuracy of 92%\n",
    "- Mean Cross Validation Score: 90.60%\n",
    "- `Naive Bayes` + `TfIdf` : Gave us the second best f1-score average accuracy of 88%\n",
    "- Mean Cross Validation Score: 87.73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cd75d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y_true, y_pred, labels=None, normalize=False, figsize=(6, 4), title=\"Confusion Matrix\"):\n",
    "    if labels is None:\n",
    "        labels = sorted(list(set(y_true) | set(y_pred)))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "        fmt = \".2f\"\n",
    "    else:\n",
    "        fmt = \"d\"\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')  # Tilt x-axis labels\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(classifier, X, y):\n",
    "    y_pred = classifier.predict(X)\n",
    "    cv_scores = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "    print(f'\\nCross-Validation Scores: {cv_scores}')\n",
    "    print(f'Mean Cross-Validation Score: {np.mean(cv_scores):.4f}')\n",
    "\n",
    "    print(\"\\n Model Evaluation\")\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    class_report = classification_report(y, y_pred, target_names=sorted(y.unique()))\n",
    "    print(class_report)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = show_confusion_matrix(y_test, y_pred, labels=sorted(y_test.unique()))\n",
    "\n",
    "print(\"SVM Classifier Evaluation\")\n",
    "model_evaluation(svm_classifier, X_test, y_test)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Naive Bayes Classifier Evaluation\")\n",
    "model_evaluation(nb_classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4d1e6",
   "metadata": {},
   "source": [
    "#### Prediction function that uses the SVM + TfIdf model\n",
    "- The funciton takes a string as input\n",
    "- Preprocess the text\n",
    "- Transform the text using the `tfidf_vectorizer`\n",
    "- Predict the category using the `svm_model`\n",
    "- Return the predicted category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c960b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text):\n",
    "    tokenized_text = preprocess_text(text)\n",
    "    X_new = tfidf_vectorizer.transform([tokenized_text])\n",
    "    return svm_classifier.predict(X_new)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33b721",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Task 2: Traditional Text Generation\n",
    "N-gram language models for Arabic text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b33316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CULTURE â†’\", predict_category(\"Ø£Ø·Ù„Ù‚Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø«Ù‚Ø§ÙØ© Ø¨Ø±Ù†Ø§Ù…Ø¬Ù‹Ø§ ÙˆØ·Ù†ÙŠÙ‹Ø§ ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø¥Ø­ÙŠØ§Ø¡ Ø§Ù„ØªØ±Ø§Ø« Ø§Ù„Ø´Ø¹Ø¨ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙˆÙ† Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© ÙˆØ§Ù„Ù…Ù‡Ø±Ø¬Ø§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„ØªÙŠ ØªØ³Ù„Ø· Ø§Ù„Ø¶ÙˆØ¡ Ø¹Ù„Ù‰ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©.\"))\n",
    "print(\"ECONOMY â†’\", predict_category(\"Ø´Ù‡Ø¯Øª Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ø±ØªÙØ§Ø¹Ù‹Ø§ Ù…Ù„Ø­ÙˆØ¸Ù‹Ø§ ÙÙŠ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£Ø³Ù‡Ù… Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø¨Ø¹Ø¯ Ø¥Ø¹Ù„Ø§Ù† Ø§Ù„Ø­ÙƒÙˆÙ…Ø© Ø¹Ù† Ø®Ø·Ø© ØªÙ†Ù…ÙˆÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© ØªØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†ÙØ·.\"))\n",
    "print(\"INTERNATIONAL â†’\", predict_category(\"Ø¹Ù‚Ø¯Øª Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ© Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠØ© Ø§Ø¬ØªÙ…Ø§Ø¹Ù‡Ø§ Ø§Ù„Ø³Ù†ÙˆÙŠ ÙÙŠ Ø¨Ø±ÙˆÙƒØ³Ù„ Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø£Ù…Ù† Ø§Ù„ØºØ°Ø§Ø¦ÙŠ ÙˆØ§Ù„ØªØºÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø®ÙŠ ÙˆØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ø§ÙˆÙ† Ø¨ÙŠÙ† Ø§Ù„Ø´Ø±Ù‚ ÙˆØ§Ù„ØºØ±Ø¨.\"))\n",
    "print(\"LOCAL â†’\", predict_category(\"Ø¨Ø¯Ø£Øª Ø£Ù…Ø§Ù†Ø© Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø¨ØªÙ†ÙÙŠØ° Ù…Ø´Ø±ÙˆØ¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø¨Ù‡Ø¯Ù ØªØ®ÙÙŠÙ Ø§Ù„Ø§Ø²Ø¯Ø­Ø§Ù… Ø§Ù„Ù…Ø±ÙˆØ±ÙŠØŒ ÙƒÙ…Ø§ ØªÙ… Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù† Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù…Ø±Ø§Øª Ù…Ø´Ø§Ø© ÙˆÙ…ÙˆØ§Ù‚Ù Ø°ÙƒÙŠØ©.\"))\n",
    "print(\"RELIGION â†’\", predict_category(\"Ø­Ø«Ù‘ Ø¥Ù…Ø§Ù… Ø§Ù„Ù…Ø³Ø¬Ø¯ Ø®Ù„Ø§Ù„ Ø®Ø·Ø¨Ø© Ø§Ù„Ø¬Ù…Ø¹Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙ…Ø³Ùƒ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆÙ†Ø´Ø± Ø§Ù„ØªØ³Ø§Ù…Ø­ Ø¨ÙŠÙ† Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ØŒ Ù…Ø´ÙŠØ±Ù‹Ø§ Ø¥Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØµØ¯Ù‚ ÙˆØ§Ù„Ø£Ù…Ø§Ù†Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©.\"))\n",
    "print(\"SPORTS â†’\", predict_category(\"ØªÙ…ÙƒÙ† Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù…Ù† Ø§Ù„ÙÙˆØ² Ø¹Ù„Ù‰ Ù†Ø¸ÙŠØ±Ù‡ Ø§Ù„Ø¥ÙŠØ±Ø§Ù†ÙŠ ÙÙŠ Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ø«ÙŠØ±Ø© Ø§Ù†ØªÙ‡Øª Ø¨Ù†ØªÙŠØ¬Ø© Ù£-Ù¢ØŒ Ù„ÙŠØ¶Ù…Ù† Ø§Ù„ØªØ£Ù‡Ù„ Ø¥Ù„Ù‰ Ù†Ù‡Ø§Ø¦ÙŠ ÙƒØ£Ø³ Ø¢Ø³ÙŠØ§ ÙˆØ³Ø· ÙØ±Ø­Ø© Ø¬Ù…Ø§Ù‡ÙŠØ±ÙŠØ© Ø¹Ø§Ø±Ù…Ø©.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f3052",
   "metadata": {},
   "source": [
    "### ðŸ” Observations On The Predictions\n",
    "* The model correctly predicted **5 out of 6** categories.\n",
    "* The **RELIGION** article was misclassified as **LOCAL**, likely due to **data imbalance** or **semantic overlap** in community-related language.\n",
    "* Overall, the model demonstrates **strong accuracy**, with minor limitations in underrepresented categories.\n",
    "\n",
    "Let me know if you want this phrased formally for a report or presentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f7147",
   "metadata": {},
   "source": [
    "### Task 2: Text Generation\n",
    "We needed to use different preprocessing step to get rid of stemming and other steps that will ruin the generation of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_arabic_text(text):\n",
    "    # Remove non-Arabic characters and normalize whitespace\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text) # keep only Arabic characters\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)  # remove Arabic diacritics\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # normalize whitespace\n",
    "    text = re.sub(r'[Ø§Ø¢Ø¥Ø£]', 'Ø§', text)  # replace Arabic letter with hamza with 'Ø§'\n",
    "\n",
    "    return text\n",
    "preprocess_arabic_text(\"!Ù…Ø±Ø­Ø¨Ø§Ù‹... Ù‡Ø°Ø§ Ù†ÙŽØµÙ‘ÙŒ ØªÙŽØ¬Ù’Ø±ÙÙŠØ¨ÙÙŠÙŒ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 123 Ø£Ø±Ù‚Ø§Ù… Ù¤Ù¥Ù¦ØŒ ÙÙŠ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ… @#$%ØŒ ÙƒÙ„Ù…Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© like This.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7c2b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to build n-gram model\n",
    "def build_ngram_model(texts, n):\n",
    "    model = defaultdict(list)\n",
    "    all_words = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Preprocess the text\n",
    "        text = preprocess_arabic_text(text)\n",
    "        # Split into words\n",
    "        words = text.split()\n",
    "        all_words.extend(words)\n",
    "        \n",
    "        # Build n-grams\n",
    "        for i in range(len(words) - n + 1):\n",
    "            # Use tuple of n-1 words as key\n",
    "            prefix = tuple(words[i:i+n-1])\n",
    "            # Use the nth word as value\n",
    "            suffix = words[i+n-1]\n",
    "            model[prefix].append(suffix)\n",
    "    \n",
    "    return model, list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae964451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Function to generate text with a random start word\n",
    "def generate_arabic_text(model, all_words, length=100, n=5):\n",
    "    # Choose a random start word\n",
    "    start_word = random.choice(all_words)\n",
    "    \n",
    "    # Find a valid prefix that contains the start word\n",
    "    valid_prefixes = [prefix for prefix in model.keys() if start_word in prefix]\n",
    "    \n",
    "    # If no valid prefix contains the start word, just use any prefix\n",
    "    if valid_prefixes:\n",
    "        current = random.choice(valid_prefixes)\n",
    "    else:\n",
    "    # Fall back to any random prefix\n",
    "        current = random.choice(list(model.keys()))\n",
    "        start_word = current[0] if len(current) > 0 else start_word  # Update start word to match what we're using\n",
    "\n",
    "    result = list(current)\n",
    "    \n",
    "    # Generate text\n",
    "    for _ in range(length):\n",
    "        if current in model:\n",
    "            # Choose a random next word based on the current n-1 words\n",
    "            next_word = random.choice(model[current])\n",
    "            result.append(next_word)\n",
    "            # Update current context (sliding window)\n",
    "            current = tuple(result[-(n-1):])\n",
    "        else:\n",
    "            # if we reach a dead end, choose a new random prefix\n",
    "            current = random.choice(list(model.keys()))\n",
    "            result.extend(current)\n",
    "    \n",
    "    return start_word, ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eee46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def build_eval_ngram_model(n, texts):\n",
    "    model, all_words = build_ngram_model(texts, n)\n",
    "    print(f\"\\nâœ… N-gram model built with n={n} ({len(model)} prefixes)\\n\")\n",
    "\n",
    "    for i in range(3):\n",
    "        start_word, generated_text = generate_arabic_text(model, all_words, length=30, n=n)\n",
    "        print(f\"ðŸ”¹ Sample {i+1} (start: '{start_word}'):\")\n",
    "        for word in generated_text.split(): \n",
    "            print(word, end=' ', flush=True)\n",
    "            time.sleep(0.05)\n",
    "        print('')  # new line after each sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c880d4",
   "metadata": {},
   "source": [
    "#### Testing the model from n=1 to n=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    build_eval_ngram_model(i+1, df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f4966",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 4. Phase 2: Modern Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cfba5",
   "metadata": {},
   "source": [
    "## Task 1: Deep Learning Text Classification\n",
    "Modern neural network approaches including BiLSTM and Transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "4f2dff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc1ac4d",
   "metadata": {},
   "source": [
    "1- **Tokenization**\n",
    "\n",
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e915406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokenized_text = [text.split() for text in df['processed_text']]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(word for article in tokenized_text for word in article)\n",
    "\n",
    "# Vocabulary\n",
    "vocab = {word: idx + 2 for idx, (word, count) in enumerate(word_counts.items())}\n",
    "vocab['<PAD>'] = 0  # Padding token\n",
    "vocab['<UNK>'] = 1  # Unknown token\n",
    "\n",
    "# Reverse vocabulary for decoding\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Stats\n",
    "print(f\"Most common words: {word_counts.most_common(10)}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67427b83",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "4de4bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d59ea",
   "metadata": {},
   "source": [
    "1.1 **Tokenizer Encoder and Decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a454d0",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f90eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(article, vocab, max_len):\n",
    "    tokens = [vocab.get(word, vocab['<UNK>']) for word in article]\n",
    "    chunks = []\n",
    "\n",
    "    # Split the tokens into chunks of max_len\n",
    "    for i in range(0, len(tokens), max_len):\n",
    "        chunk = tokens[i:i + max_len]\n",
    "        if len(chunk) < max_len:\n",
    "            chunk += [vocab['<PAD>']] * (max_len - len(chunk))\n",
    "        chunks.append(chunk)\n",
    "        # break # Act as the normal encode function, not chunking\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def decode_text(encoded_article, reverse_vocab):\n",
    "    return ' '.join(reverse_vocab.get(idx, '<UNK>') for idx in encoded_article if idx not in (0, 1))  # Skip PAD and UNK tokens\n",
    "\n",
    "# Test encoding and decoding\n",
    "sample_article = tokenized_text[0]\n",
    "print(f\"Sample article encoded and decoded safely: {' '.join([decode_text(chunk, reverse_vocab) for chunk in encode_text(sample_article, vocab, 100)]) == ' '.join(sample_article)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd6b40",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "a55f27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_transformer(text, max_len=128):\n",
    "    assert max_len < 512, \"Max length for BERT should be less than 512 tokens.\"\n",
    "    \n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_len - 2):\n",
    "        chunk = tokens[i:i + (max_len - 2)]\n",
    "\n",
    "        chunk = [2] + chunk + [3]  # Add [CLS] and [SEP] tokens\n",
    "\n",
    "        padding_length = max_len - len(chunk)\n",
    "        chunk += [0] * padding_length  # Pad with zeros\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def decode_text_transformer(encoded_article):\n",
    "    decoded = tokenizer.decode(encoded_article, skip_special_tokens=True)\n",
    "    return decoded.replace('  ', ' ').strip()  # Clean up double spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…\", max_length=24, truncation=True, padding='max_length'))\n",
    "print(encode_text_transformer(\"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…\", max_len=12))\n",
    "print(decode_text_transformer(encode_text_transformer(\"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…\")[0]) == \"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…\")\n",
    "# There will be less padding in the encode_text_transformer because chunking adds CLS and SEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b95201f",
   "metadata": {},
   "source": [
    "1.2 **Label Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}\n",
    "label2id = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
    "decode_labels = lambda idx: label_encoder.inverse_transform(idx)\n",
    "print(f\"Label mapping: {label_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ad6db",
   "metadata": {},
   "source": [
    "2- **Dataset Preparation**\n",
    "\n",
    "2.1- **Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "219b53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        self.lengths = (self.texts != vocab['<PAD>']).sum(dim=1)  # Calculate lengths for each text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.lengths[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b35f5",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "1d19f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTextDataset(Dataset):\n",
    "    def __init__(self, tokenized_text, labels, tokenizer, max_len, split = 'train'):\n",
    "        self.tokenized_text = torch.tensor(tokenized_text, dtype=torch.long)\n",
    "        self.attention_mask = self.tokenized_text != 0\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_text[idx]\n",
    "        # if self.split == 'test':\n",
    "        #     input_ids = decode_text_transformer(input_ids)\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d279b7e",
   "metadata": {},
   "source": [
    "3- **Model Architecture**\n",
    "\n",
    "3.1- **LSTM Model**: Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "9abf4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, pad_idx):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, num_layers=max(2, num_layers//2), dropout=0.4, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # normalization layer\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "        # fc layer\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "\n",
    "        # Second BiLSTM layer\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=max(2, num_layers//2), dropout=0.4, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # normalization layer\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Pack the sequence for LSTM\n",
    "        packed1 = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out1, _ = self.lstm1(packed1)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        lstm1_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out1, batch_first=True)\n",
    "\n",
    "        lstm1_out = self.norm1(lstm1_out) # (batch_size, seq_len, hidden_dim * 2)\n",
    "        fc1_out = self.fc1(lstm1_out) # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Pack the sequence for the second LSTM\n",
    "        packed2 = nn.utils.rnn.pack_padded_sequence(\n",
    "            fc1_out, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out2, (hidden, _) = self.lstm2(packed2)\n",
    "        lstm2_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out2, batch_first=True)\n",
    "        \n",
    "        lstm2_out = self.norm2(lstm2_out + lstm1_out) # Residual connection\n",
    "\n",
    "        # Use the final forward and backward hidden states\n",
    "        out = torch.cat((hidden[-2], hidden[-1]), dim=1)  # (batch_size, hidden_dim * 2)\n",
    "        out = self.dropout2(out)\n",
    "        return self.fc2(out)  # (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc6980",
   "metadata": {},
   "source": [
    "3.2- **Transformer Based Model**: AraBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "3e44e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# The model can be used like this:\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_mapping), id2label=label_mapping, label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d1fd1",
   "metadata": {},
   "source": [
    "4- **Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "86c64c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde72b3",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "97df8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 100  # Evaluate every 100 batches\n",
    "eval_iters = 10 # Number of iterations for evaluation\n",
    "\n",
    "max_len = 500  # Maximum length of sequences\n",
    "batch_size = 256 # Batch size for training\n",
    "bilstm_num_epochs = 30  # Number of epochs for BiLSTM training\n",
    "lr = 2e-3  # Learning rate\n",
    "\n",
    "embedding_dim = 300  # Dimension of word embeddings\n",
    "num_layers = 6  # Number of LSTM layers\n",
    "hidden_dim = 256  # Hidden dimension for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e38aaf",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "25bcc28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './results'\n",
    "evaluation_strategy = 'epoch' # # Evaluate at the end of each epoch\n",
    "save_strategy = 'epoch'  # Save model at the end of each epoch\n",
    "tf_learning_rate = 4e-5 # 1e-4\n",
    "per_device_train_batch_size = 256\n",
    "per_device_eval_batch_size = 32\n",
    "gradient_accumulation_steps = 2\n",
    "num_train_epochs = 8\n",
    "weight_decay = 0.01\n",
    "logging_dir = \"./logs\"\n",
    "load_best_model_at_end = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c92934",
   "metadata": {},
   "source": [
    "4.1- **Data Loaders**\n",
    "\n",
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1712cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_texts_BiLSTM = []\n",
    "chunked_labels_BiLSTM = []\n",
    "\n",
    "\n",
    "for article, label in zip(tokenized_text, encoded_labels):\n",
    "    chunksBiLSTM = encode_text(article, vocab, max_len)\n",
    "\n",
    "    chunked_texts_BiLSTM.extend(chunksBiLSTM)\n",
    "    chunked_labels_BiLSTM.extend([label] * len(chunksBiLSTM))\n",
    "\n",
    "X_train, X_devtest, y_train, y_devtest = train_test_split(\n",
    "    chunked_texts_BiLSTM, chunked_labels_BiLSTM, test_size=0.2, stratify=chunked_labels_BiLSTM, random_state=42\n",
    ")\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_devtest, y_devtest, test_size=0.5, stratify=y_devtest, random_state=42\n",
    ")\n",
    "\n",
    "biLSTMtrain_dataset = BiLSTMTextDataset(X_train, y_train)\n",
    "biLSTMdev_dataset = BiLSTMTextDataset(X_dev, y_dev)\n",
    "biLSTMtest_dataset = BiLSTMTextDataset(X_test, y_test)\n",
    "\n",
    "bitrain_loader = DataLoader(biLSTMtrain_dataset, batch_size=batch_size, shuffle=True)\n",
    "bidev_loader = DataLoader(biLSTMdev_dataset, batch_size=batch_size, shuffle=False)\n",
    "bitest_loader = DataLoader(biLSTMtest_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# stats\n",
    "print(f\"BiLSTM Train dataset size: {len(biLSTMtrain_dataset)}\")\n",
    "print(f\"BiLSTM Dev dataset size: {len(biLSTMdev_dataset)}\")\n",
    "print(f\"BiLSTM Test dataset size: {len(biLSTMtest_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2de3c",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabert preprocessing\n",
    "df['arabert_text'] = df['text'].apply(arabert_prep.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_texts_Transformer = []\n",
    "chunked_labels_Transformer = []\n",
    "\n",
    "for article, label in zip(df['arabert_text'].tolist(), encoded_labels):\n",
    "    chucksTransformer = encode_text_transformer(article, max_len=max_len)\n",
    "\n",
    "    chunked_texts_Transformer.extend(chucksTransformer)\n",
    "    chunked_labels_Transformer.extend([label] * len(chucksTransformer))\n",
    "\n",
    "X_train, X_devtest, y_train, y_devtest = train_test_split(\n",
    "    chunked_texts_Transformer, chunked_labels_Transformer, test_size=0.2, stratify=chunked_labels_Transformer, random_state=42\n",
    ")\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_devtest, y_devtest, test_size=0.5, stratify=y_devtest, random_state=42\n",
    ")\n",
    "\n",
    "tFtrain_dataset = TransformerTextDataset(X_train, y_train, tokenizer, max_len)\n",
    "tFdev_dataset = TransformerTextDataset(X_dev, y_dev, tokenizer, max_len)\n",
    "tFtest_dataset = TransformerTextDataset(X_test, y_test, tokenizer, max_len, split='test')\n",
    "\n",
    "tFtrain_loader = DataLoader(tFtrain_dataset, batch_size=per_device_train_batch_size, shuffle=True)\n",
    "tFdev_loader = DataLoader(tFdev_dataset, batch_size=per_device_train_batch_size, shuffle=False)\n",
    "tFtest_loader = DataLoader(tFtest_dataset, batch_size=per_device_eval_batch_size, shuffle=False)\n",
    "\n",
    "# stats\n",
    "print(f\"Transformer Train dataset size: {len(tFtrain_dataset)}\")\n",
    "print(f\"Transformer Dev dataset size: {len(tFdev_dataset)}\")\n",
    "print(f\"Transformer Test dataset size: {len(tFtest_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16bc627",
   "metadata": {},
   "source": [
    "4.1- **Evaluation Function**: To evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "2a3ee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, transformer=False):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader: # x: (batch_size, seq_len), y: (batch_size,)\n",
    "\n",
    "            if transformer:\n",
    "                print(f\"{cnt}/{len(test_loader)}\", end='\\r')\n",
    "                x = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                y = batch['labels'].to(device)\n",
    "                outputs = model(x, attention_mask=attention_mask)  # outputs: (batch_size, num_classes)\n",
    "                outputs = outputs.logits\n",
    "            else:\n",
    "                x = batch[0].to(device)\n",
    "                lengths = batch[1].to(device)\n",
    "                y = batch[2].to(device)\n",
    "                outputs = model(x, lengths)  # outputs: (batch_size, num_classes)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            cnt += 1\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d3caf",
   "metadata": {},
   "source": [
    "4.2- **Model Training**: train_model function to train given a model, loader, criterion, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "9e7e7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    step = 0\n",
    "\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, lengths, y in train_loader: # (x: (batch_size, seq_len), y: (batch_size,))\n",
    "        x, lengths, y = x.to(device), lengths.to(device), y.to(device)\n",
    "        # Forward pass:\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(x, lengths)\n",
    "            loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update running loss & accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        acc = (preds == y).float().mean().item()\n",
    "        running_acc = running_acc * 0.90 + acc * 0.10 if running_acc > 0 else acc\n",
    "        running_loss = running_loss * 0.90 + loss.item() * 0.10 if running_loss > 0 else loss.item()\n",
    "        step += 1\n",
    "\n",
    "        print(f\"step {step:4d} | loss: {running_loss:.6f} | acc: {running_acc:.6f}\", end='\\r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15beecd",
   "metadata": {},
   "source": [
    "4.3- **Training Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971d8e5",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6bb5d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "# Learning rate scheduler\n",
    "\n",
    "warmup_steps = 60\n",
    "total_steps = len(bitrain_loader) * bilstm_num_epochs\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.01 + 0.99 * 0.5 * (1.0 + math.cos(math.pi * progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "81f3db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBiLSTM = BiLSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,  # Embedding dimension\n",
    "    hidden_dim=hidden_dim,  # Hidden dimension for LSTM\n",
    "    num_layers=num_layers,  # Number of LSTM layers\n",
    "    num_classes=len(label_mapping),  # Number of classes\n",
    "    pad_idx=vocab['<PAD>']  # Padding index\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(modelBiLSTM.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087192ca",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTF = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_mapping),\n",
    "    id2label=label_mapping,\n",
    "    label2id=label2id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead26e5b",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "d1d20e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use high precision for float32 matrix multiplication to improve performance\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "modelBiLSTM = torch.compile(modelBiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ff271",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(bilstm_num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    train_model(modelBiLSTM, bitrain_loader, criterion, optimizer, scheduler, device)\n",
    "    y_pred, y_true = evaluate_model(modelBiLSTM, bidev_loader, device)\n",
    "    acc = (np.array(y_pred) == np.array(y_true)).mean()\n",
    "    print(f\"validation accuracy: {acc:.4f}, f1: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Updated learning rate: {param_group['lr']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "ed7c440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(modelBiLSTM.state_dict(), \"bilstm_best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ec1c7",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c845dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=evaluation_strategy,\n",
    "    save_strategy=save_strategy,\n",
    "    learning_rate=tf_learning_rate,\n",
    "    bf16=True,  # instead of fp16\n",
    "    fp16=False,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir=logging_dir,\n",
    "    load_best_model_at_end=load_best_model_at_end\n",
    "    # torch_compile=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=modelTF,\n",
    "    args=training_args,\n",
    "    train_dataset=tFtrain_dataset,\n",
    "    eval_dataset=tFdev_dataset,\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': (np.argmax(p.predictions, axis=1) == p.label_ids).mean(),\n",
    "        'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted')\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "del modelBiLSTM\n",
    "del modelTF\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fccd05b",
   "metadata": {},
   "source": [
    "5- **Evaluation And Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b22c3",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Evaluation (On Test Set):\")\n",
    "y_pred, y_true = evaluate_model(modelBiLSTM, bitest_loader, device)\n",
    "acc = (np.array(y_pred) == np.array(y_true)).mean()\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "show_confusion_matrix(decode_labels(y_true), decode_labels(y_pred), labels=sorted(set(decode_labels(y_true)) | set(decode_labels(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_predict(text, model, vocab, max_len=500):\n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_text(text)\n",
    "    tokens = processed_text.split()\n",
    "    \n",
    "    # Encode the text\n",
    "    encoded = [vocab.get(word, vocab['<UNK>']) for word in tokens]\n",
    "    \n",
    "    # Pad or truncate to max_len\n",
    "    if len(encoded) < max_len:\n",
    "        encoded += [vocab['<PAD>']] * (max_len - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:max_len]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor([encoded], dtype=torch.long).to(device)\n",
    "    length_tensor = torch.tensor([len(tokens)], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor, length_tensor)\n",
    "        prediction = torch.argmax(outputs, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    return label_encoder.inverse_transform([prediction])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ec2cd",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "14ae1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "checkpoint_path = \"results/checkpoint-288-best\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdbe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Evaluation (On Test Set):\")\n",
    "y_pred, y_true = evaluate_model(model, tFtest_loader, device, transformer=True)\n",
    "acc = (np.array(y_pred) == np.array(y_true)).mean()\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "show_confusion_matrix(decode_labels(y_true), decode_labels(y_pred), labels=sorted(set(decode_labels(y_true)) | set(decode_labels(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabert_predict(text, model, tokenizer, max_len=500):\n",
    "    # Preprocess the text using AraBERT preprocessing\n",
    "    processed_text = arabert_prep.preprocess(text)\n",
    "    \n",
    "    # Tokenize and encode\n",
    "    tokens = tokenizer.encode(processed_text, add_special_tokens=True, \n",
    "                             max_length=max_len, truncation=True, padding='max_length')\n",
    "    attention_mask = [1 if token != 0 else 0 for token in tokens]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "    attention_mask_tensor = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask_tensor)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    return label_encoder.inverse_transform([prediction])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b953d12",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 5. Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f1481",
   "metadata": {},
   "source": [
    "### 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1106138",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 7. Bonus: Interactive Demo\n",
    "\n",
    "### Interactive Text Classification Demo\n",
    "Here we demonstrate all our trained models by testing them on sample Arabic texts from different categories. Each model uses its own preprocessing and prediction pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92a559",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Model Comparison on Sample Texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test samples for each category\n",
    "test_samples = [\n",
    "    (\"Ø£Ø·Ù„Ù‚Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø«Ù‚Ø§ÙØ© Ø¨Ø±Ù†Ø§Ù…Ø¬Ù‹Ø§ ÙˆØ·Ù†ÙŠÙ‹Ø§ ÙŠÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ø¥Ø­ÙŠØ§Ø¡ Ø§Ù„ØªØ±Ø§Ø« Ø§Ù„Ø´Ø¹Ø¨ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ Ø¯Ø¹Ù… Ø§Ù„ÙÙ†ÙˆÙ† Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© ÙˆØ§Ù„Ù…Ù‡Ø±Ø¬Ø§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„ØªÙŠ ØªØ³Ù„Ø· Ø§Ù„Ø¶ÙˆØ¡ Ø¹Ù„Ù‰ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©.\", \"CULTURE\"),\n",
    "    (\"Ø´Ù‡Ø¯Øª Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ø±ØªÙØ§Ø¹Ù‹Ø§ Ù…Ù„Ø­ÙˆØ¸Ù‹Ø§ ÙÙŠ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø£Ø³Ù‡Ù… Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø¨Ø¹Ø¯ Ø¥Ø¹Ù„Ø§Ù† Ø§Ù„Ø­ÙƒÙˆÙ…Ø© Ø¹Ù† Ø®Ø·Ø© ØªÙ†Ù…ÙˆÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© ØªØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠ ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†ÙØ·.\", \"ECONOMY\"),\n",
    "    (\"Ø¹Ù‚Ø¯Øª Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ© Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠØ© Ø§Ø¬ØªÙ…Ø§Ø¹Ù‡Ø§ Ø§Ù„Ø³Ù†ÙˆÙŠ ÙÙŠ Ø¨Ø±ÙˆÙƒØ³Ù„ Ù„Ù…Ù†Ø§Ù‚Ø´Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø£Ù…Ù† Ø§Ù„ØºØ°Ø§Ø¦ÙŠ ÙˆØ§Ù„ØªØºÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø®ÙŠ ÙˆØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ø§ÙˆÙ† Ø¨ÙŠÙ† Ø§Ù„Ø´Ø±Ù‚ ÙˆØ§Ù„ØºØ±Ø¨.\", \"INTERNATIONAL\"),\n",
    "    (\"Ø¨Ø¯Ø£Øª Ø£Ù…Ø§Ù†Ø© Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø¨ØªÙ†ÙÙŠØ° Ù…Ø´Ø±ÙˆØ¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø¨Ù‡Ø¯Ù ØªØ®ÙÙŠÙ Ø§Ù„Ø§Ø²Ø¯Ø­Ø§Ù… Ø§Ù„Ù…Ø±ÙˆØ±ÙŠØŒ ÙƒÙ…Ø§ ØªÙ… Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù† Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù…Ø±Ø§Øª Ù…Ø´Ø§Ø© ÙˆÙ…ÙˆØ§Ù‚Ù Ø°ÙƒÙŠØ©.\", \"LOCAL\"),\n",
    "    (\"Ø­Ø«Ù‘ Ø¥Ù…Ø§Ù… Ø§Ù„Ù…Ø³Ø¬Ø¯ Ø®Ù„Ø§Ù„ Ø®Ø·Ø¨Ø© Ø§Ù„Ø¬Ù…Ø¹Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙ…Ø³Ùƒ Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆÙ†Ø´Ø± Ø§Ù„ØªØ³Ø§Ù…Ø­ Ø¨ÙŠÙ† Ø£ÙØ±Ø§Ø¯ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ØŒ Ù…Ø´ÙŠØ±Ù‹Ø§ Ø¥Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØµØ¯Ù‚ ÙˆØ§Ù„Ø£Ù…Ø§Ù†Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©.\", \"RELIGION\"),\n",
    "    (\"ØªÙ…ÙƒÙ† Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù…Ù† Ø§Ù„ÙÙˆØ² Ø¹Ù„Ù‰ Ù†Ø¸ÙŠØ±Ù‡ Ø§Ù„Ø¥ÙŠØ±Ø§Ù†ÙŠ ÙÙŠ Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ø«ÙŠØ±Ø© Ø§Ù†ØªÙ‡Øª Ø¨Ù†ØªÙŠØ¬Ø© Ù£-Ù¢ØŒ Ù„ÙŠØ¶Ù…Ù† Ø§Ù„ØªØ£Ù‡Ù„ Ø¥Ù„Ù‰ Ù†Ù‡Ø§Ø¦ÙŠ ÙƒØ£Ø³ Ø¢Ø³ÙŠØ§ ÙˆØ³Ø· ÙØ±Ø­Ø© Ø¬Ù…Ø§Ù‡ÙŠØ±ÙŠØ© Ø¹Ø§Ø±Ù…Ø©.\", \"SPORTS\")\n",
    "]\n",
    "\n",
    "print(\"ðŸ” INTERACTIVE MODEL COMPARISON DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (text, expected) in enumerate(test_samples, 1):\n",
    "    print(f\"\\nðŸ“ Sample {i} (Expected: {expected}):\")\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    print(\"\\nðŸ”® Predictions:\")\n",
    "    \n",
    "    # Traditional method prediction\n",
    "    traditional_pred = predict_category(text)\n",
    "    print(f\"   Traditional (SVM): {traditional_pred} {'âœ…' if traditional_pred == expected else 'âŒ'}\")\n",
    "    \n",
    "    # BiLSTM prediction\n",
    "    bilstm_pred = lstm_predict(text, modelBiLSTM, vocab)\n",
    "    print(f\"   BiLSTM: {bilstm_pred} {'âœ…' if bilstm_pred == expected else 'âŒ'}\")\n",
    "    \n",
    "    # AraBERT prediction  \n",
    "    arabert_pred = arabert_predict(text, model, tokenizer)\n",
    "    print(f\"   AraBERT: {arabert_pred} {'âœ…' if arabert_pred == expected else 'âŒ'}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afc775",
   "metadata": {},
   "source": [
    "### Task 2: Arabic Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8fefd9",
   "metadata": {},
   "source": [
    "### 2.2 Traditional Approach: Seq2Seq with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c76c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess Arabic summarization dataset\n",
    "df_sum = pd.read_excel(\"Text summarization dataset.xlsx\")\n",
    "df_sum = df_sum.iloc[1:].reset_index(drop=True)\n",
    "df_sum.columns = ['summary', 'text']\n",
    "df_sum = df_sum.dropna(subset=['summary', 'text'])\n",
    "df_sum.head()\n",
    "df_sum.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c49a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n",
    "    text = re.sub(r'Ù‰', 'ÙŠ', text)\n",
    "    text = re.sub(r'Ø¤', 'Ùˆ', text)\n",
    "    text = re.sub(r'Ø¦', 'ÙŠ', text)\n",
    "    text = re.sub(r'Ø©', 'Ù‡', text)\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)  # remove Arabic diacritics\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply normalization\n",
    "df_sum['text'] = df_sum['text'].apply(normalize_arabic)\n",
    "df_sum['summary'] = df_sum['summary'].apply(normalize_arabic)\n",
    "print(f\"After preprocessing: {df_sum.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52df6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Prepare sequences\n",
    "def prepare_sequences(df_sum, max_text_len=128, max_summary_len=32):\n",
    "    texts = df_sum['text']\n",
    "    summaries = df_sum['summary']\n",
    "    all_texts = list(texts) + list(summaries)\n",
    "    tokenizer = Tokenizer(filters='', oov_token=None)\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    \n",
    "    # Add special tokens\n",
    "    tokenizer.word_index['<sos>'] = len(tokenizer.word_index) + 1\n",
    "    tokenizer.word_index['<eos>'] = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    summary_sequences = tokenizer.texts_to_sequences(summaries)\n",
    "    \n",
    "    # Add <sos> and <eos> to summaries\n",
    "    summary_sequences = [[tokenizer.word_index['<sos>']] + seq + [tokenizer.word_index['<eos>']] \n",
    "                        for seq in summary_sequences]\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_padded = pad_sequences(text_sequences, maxlen=max_text_len, padding='post')\n",
    "    summary_padded = pad_sequences(summary_sequences, maxlen=max_summary_len, padding='post')\n",
    "    \n",
    "    return tokenizer, text_padded, summary_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, texts, summaries):\n",
    "        self.texts = texts          # Ø®Ù„Ù‡ list Ø§Ùˆ numpy\n",
    "        self.summaries = summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "            torch.tensor(self.summaries[idx], dtype=torch.long),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef654d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Traditional Seq2Seq Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, x, hidden, cell):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, hidden_dim)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, hidden_dim)\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(device)\n",
    "        _, (hidden, cell) = self.encoder(src)\n",
    "        input = trg[:, 0].unsqueeze(1)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(2)\n",
    "            input = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76a241",
   "metadata": {},
   "source": [
    "Evaluation funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ec589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_seq2seq_rouge(model, loader, tokenizer, device, max_len=128):\n",
    "    model.eval()\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "    totals = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in loader:                       # src, trg: (B, T)\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù„Ø®Øµ (Ù…Ù† Ø¯ÙˆÙ† teacher forcing)\n",
    "            out = model(src, trg[:, :1], teacher_forcing_ratio=0.)   # (B, T, V)\n",
    "            gen_ids = out.argmax(-1)                                # (B, T)\n",
    "\n",
    "            # IDs â†’ Ù†Øµ\n",
    "            preds = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "            refs  = tokenizer.batch_decode(trg,    skip_special_tokens=True)\n",
    "\n",
    "            # ROUGE Ù„ÙƒÙ„ Ù…Ø«Ø§Ù„\n",
    "            for p, r in zip(preds, refs):\n",
    "                s = scorer.score(r, p)\n",
    "                for k in totals: totals[k] += s[k].fmeasure\n",
    "                n += 1\n",
    "\n",
    "    return {k: v / n for k, v in totals.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77d1e2",
   "metadata": {},
   "source": [
    "training lop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051b789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq_model(model, train_loader, optimizer, criterion, scaler, epoch):\n",
    "    model.train()\n",
    "    step = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for src, trg in train_loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(src, trg)\n",
    "            # Reshape output and target for loss calculation\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss = running_loss * 0.90 + loss.item() * 0.10 if running_loss > 0 else loss.item()\n",
    "        step += 1\n",
    "\n",
    "        print(f\"step {step:4d} | loss: {running_loss:.6f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04823c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "seq2seq_max_text_len = 128\n",
    "seq2seq_max_summary_len = 32\n",
    "seq2seq_batch_size = 128\n",
    "seq2seq_epochs = 10\n",
    "seq2seq_learning_rate = 1e-4\n",
    "seq2seq_embedding_dim = 256\n",
    "seq2seq_hidden_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78364c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, text_padded, summary_padded = prepare_sequences(df_sum, max_text_len=seq2seq_max_text_len, max_summary_len=seq2seq_max_summary_len)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17de8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_devtest, y_train, y_devtest = train_test_split(\n",
    "    text_padded, summary_padded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_devtest, y_devtest, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "seq2seq_train_dataset = SummarizationDataset(X_train, y_train)\n",
    "seq2seq_dev_dataset = SummarizationDataset(X_dev, y_dev)\n",
    "seq2seq_test_dataset = SummarizationDataset(X_test, y_test)\n",
    "\n",
    "seq2seq_train_loader = DataLoader(seq2seq_train_dataset, batch_size=seq2seq_batch_size, shuffle=False)\n",
    "seq2seq_dev_loader = DataLoader(seq2seq_dev_dataset, batch_size=seq2seq_batch_size, shuffle=False)\n",
    "seq2seq_test_loader = DataLoader(seq2seq_test_dataset, batch_size=seq2seq_batch_size, shuffle=False)\n",
    "\n",
    "# stats\n",
    "print(f\"Seq2Seq Train dataset size: {len(seq2seq_train_dataset)}\")\n",
    "print(f\"Seq2Seq Dev dataset size: {len(seq2seq_dev_dataset)}\")\n",
    "print(f\"Seq2Seq Test dataset size: {len(seq2seq_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler\n",
    "\n",
    "modelSeq2Seq = Seq2Seq(\n",
    "    vocab_size=vocab_size, \n",
    "    embedding_dim=seq2seq_embedding_dim, \n",
    "    hidden_dim=seq2seq_hidden_dim\n",
    "    ).to(device)\n",
    "\n",
    "# Training setup\n",
    "seq2seq_criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "seq2seq_optimizer = optim.Adam(modelSeq2Seq.parameters(), lr=seq2seq_learning_rate, weight_decay=1e-5)\n",
    "seq2seq_scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(seq2seq_epochs):\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    train_seq2seq_model(modelSeq2Seq, seq2seq_train_loader, seq2seq_optimizer, seq2seq_criterion, seq2seq_scaler, epoch)\n",
    "    rouge = evaluate_seq2seq_rouge(modelSeq2Seq, seq2seq_dev_loader, tokenizer, device)\n",
    "    print(f\"ROUGE-1: {rouge['rouge1']:.4f} | \"\n",
    "          f\"ROUGE-2: {rouge['rouge2']:.4f} | \"\n",
    "          f\"ROUGE-L: {rouge['rougeL']:.4f}\")\n",
    "torch.save(modelSeq2Seq.state_dict(), \"arabic_summarizer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc23028",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.2 Modern Approach: Transformer-Based (AraBART)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09ba52",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation of both approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cffa6ed",
   "metadata": {},
   "source": [
    "Loading the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSeq2Seq.load_state_dict(torch.load(\"arabic_summarizer.pth\"))\n",
    "# modelTfSeq2Seq = AutoModel ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the traditional approach\n",
    "rouge = evaluate_seq2seq_rouge(modelSeq2Seq, seq2seq_dev_loader, tokenizer, device)\n",
    "print(f\"ROUGE-1: {rouge['rouge1']:.4f} | \"\n",
    "      f\"ROUGE-2: {rouge['rouge2']:.4f} | \"\n",
    "      f\"ROUGE-L: {rouge['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db769f81",
   "metadata": {},
   "source": [
    "### 2.4 Summarization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136afa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "ØªØ¹ØªØ¨Ø± Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø³Ù„ÙŠÙ…Ø© Ø£Ø³Ø§Ø³ Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¬ÙŠØ¯Ø©. ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªÙ†ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ø·Ø¹Ù…Ø© Ø§Ù„Ù…ØºØ°ÙŠØ©. Ø§Ù„Ø®Ø¶Ø±ÙˆØ§Øª ÙˆØ§Ù„ÙÙˆØ§ÙƒÙ‡ Ø§Ù„Ø·Ø§Ø²Ø¬Ø© ØªÙˆÙØ± Ø§Ù„ÙÙŠØªØ§Ù…ÙŠÙ†Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù† Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù„Ù„Ø¬Ø³Ù…. Ø§Ù„Ø¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù„Ø­ÙˆÙ… ÙˆØ§Ù„Ø£Ø³Ù…Ø§Ùƒ ÙˆØ§Ù„Ø¨Ù‚ÙˆÙ„ÙŠØ§Øª ØªØ³Ø§Ø¹Ø¯ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¹Ø¶Ù„Ø§Øª ÙˆØ¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ù†Ø³Ø¬Ø©.\n",
    "\n",
    "Ù…Ù† Ø§Ù„Ù…Ù‡Ù… ØªÙ†Ø§ÙˆÙ„ ÙˆØ¬Ø¨Ø§Øª Ù…Ù†ØªØ¸Ù…Ø© ÙˆØªØ¬Ù†Ø¨ Ø§Ù„ÙˆØ¬Ø¨Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ø§Ù„ØºÙ†ÙŠØ© Ø¨Ø§Ù„Ø¯Ù‡ÙˆÙ† ÙˆØ§Ù„Ø³ÙƒØ±ÙŠØ§Øª. Ø´Ø±Ø¨ Ø§Ù„Ù…Ø§Ø¡ Ø¨ÙƒÙ…ÙŠØ§Øª ÙƒØ§ÙÙŠØ© ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø±Ø·ÙˆØ¨Ø© Ø§Ù„Ø¬Ø³Ù… ÙˆØªØ­Ø³ÙŠÙ† Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ù‡Ø¶Ù…. ÙŠØ¬Ø¨ Ø£ÙŠØ¶Ø§Ù‹ Ø§Ù„ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¨Ø§Øª Ø§Ù„ØºØ§Ø²ÙŠØ© ÙˆØ§Ù„Ø¹ØµØ§Ø¦Ø± Ø§Ù„Ù…Ø­Ù„Ø§Ø©.\n",
    "\n",
    "ØªÙ†Ø§ÙˆÙ„ ÙˆØ¬Ø¨Ø© Ø§Ù„Ø¥ÙØ·Ø§Ø± ÙŠØ¹ØªØ¨Ø± Ù…Ù† Ø£Ù‡Ù… Ø§Ù„Ø¹Ø§Ø¯Ø§Øª Ø§Ù„ØµØ­ÙŠØ©. ÙÙ‡ÙŠ ØªÙ…Ø¯ Ø§Ù„Ø¬Ø³Ù… Ø¨Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ø¨Ø¯Ø¡ Ø§Ù„ÙŠÙˆÙ… Ø¨Ù†Ø´Ø§Ø·. Ù…Ù† Ø§Ù„Ù…Ù‡Ù… Ø£ÙŠØ¶Ø§Ù‹ ØªÙ†Ø§ÙˆÙ„ ÙˆØ¬Ø¨Ø§Øª Ø®ÙÙŠÙØ© ØµØ­ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„ÙˆØ¬Ø¨Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø·Ø§Ù‚Ø© ÙÙŠ Ø§Ù„Ø¬Ø³Ù….\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, text, max_length=128):\n",
    "    model.eval()\n",
    "    text = normalize_arabic(text)\n",
    "    text_seq = tokenizer.texts_to_sequences([text])[0]\n",
    "    text_padded = pad_sequences([text_seq], maxlen=seq2seq_max_text_len, padding='post')\n",
    "    text_tensor = torch.tensor(text_padded, dtype=torch.long).to(device)\n",
    "    sos_idx = tokenizer.word_index.get('<sos>', 2)\n",
    "    eos_idx = tokenizer.word_index.get('<eos>', 3)\n",
    "    decoder_input = torch.tensor([[sos_idx]], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        _, (hidden, cell) = model.encoder(text_tensor)\n",
    "    summary = []\n",
    "    for _ in range(max_length):\n",
    "        output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
    "        predicted = output.argmax(2)\n",
    "        pred_idx = predicted.item()\n",
    "        if pred_idx == eos_idx:\n",
    "            break\n",
    "        summary.append(pred_idx)\n",
    "        decoder_input = predicted\n",
    "    idx2word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    summary_words = [idx2word.get(idx, '') for idx in summary]\n",
    "    return ' '.join(summary_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980e530",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Instructions for Deployment and Usage\n",
    "\n",
    "**To use the trained models independently:**\n",
    "\n",
    "1. **Load the saved models:**\n",
    "   ```python\n",
    "   # For traditional models  \n",
    "   svm_model = joblib.load('svm_model.pkl')\n",
    "   tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "   \n",
    "   # For BiLSTM\n",
    "   bilstm_model.load_state_dict(torch.load('bilstm_best_model.pth'))\n",
    "   \n",
    "   # For AraBERT\n",
    "   arabert_model = AutoModelForSequenceClassification.from_pretrained('results/checkpoint-288-best')\n",
    "   ```\n",
    "\n",
    "2. **Use the prediction functions:**\n",
    "   ```python\n",
    "   # Traditional\n",
    "   result = predict_category(\"Ù†Øµ Ø¹Ø±Ø¨ÙŠ\")\n",
    "   \n",
    "   # BiLSTM  \n",
    "   result = lstm_predict(\"Ù†Øµ Ø¹Ø±Ø¨ÙŠ\", modelBiLSTM, vocab)\n",
    "   \n",
    "   # AraBERT\n",
    "   result = arabert_predict(\"Ù†Øµ Ø¹Ø±Ø¨ÙŠ\", model, tokenizer)\n",
    "   ```\n",
    "\n",
    "3. **For web deployment:** Create a Flask/FastAPI wrapper around these functions\n",
    "\n",
    "**System Requirements:**\n",
    "- Python 3.8+, PyTorch 1.12+, Transformers 4.21+, scikit-learn 1.1+, NLTK with Arabic stopwords\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
