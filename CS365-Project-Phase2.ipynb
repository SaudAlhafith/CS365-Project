{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df75be8",
   "metadata": {},
   "source": [
    "# CS365 Project Phase 2: Arabic NLP Classification\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### Project Overview and Objectives\n",
    "This project implements both traditional and modern approaches for Arabic text classification using the KalimatCorpus-2.0 dataset. The main objectives are:\n",
    "- Implement traditional machine learning approaches (SVM, Naive Bayes with TF-IDF)\n",
    "- Implement modern deep learning approaches (BiLSTM, AraBERT)\n",
    "- Compare performance across different methods\n",
    "- Analyze trade-offs between traditional and modern approaches\n",
    "\n",
    "### Dataset Description\n",
    "The chosen dataset is KalimatCorpus-2.0, which contains Arabic news articles from multiple categories:\n",
    "- **Culture**: Cultural news and articles\n",
    "- **Economy**: Economic and business news\n",
    "- **International**: International news coverage\n",
    "- **Local**: Local Omani news\n",
    "- **Religion**: Religious content and discussions\n",
    "- **Sports**: Sports news and coverage\n",
    "\n",
    "### Background on Arabic NLP Challenges\n",
    "Arabic NLP presents unique challenges including:\n",
    "- Right-to-left script direction\n",
    "- Complex morphology and root-based word formation\n",
    "- Diacritics and various letter forms\n",
    "- Dialectal variations and Modern Standard Arabic differences\n",
    "- Limited preprocessing tools compared to English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7163b8e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Data Exploration and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099170f",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Downloading each article category in KalimatCorpus-2.0 from sourceforge.net\n",
    "- Each category is in a folder, with all articles as .txt\n",
    "- All articles are written words each in a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1681a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing.dummy as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe1b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat_base = \"data/KalimatCorpus-2.0\"\n",
    "expected_dirs = os.listdir(kalimat_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcc2fcc",
   "metadata": {},
   "source": [
    "#### Checking for the existence of the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_kalimat_structure_os():\n",
    "    missing = [d for d in expected_dirs if not os.path.isdir(os.path.join(kalimat_base, d))]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"❌ Missing folders: {missing}\")\n",
    "    else:\n",
    "        count = 0\n",
    "        for d in expected_dirs:\n",
    "            folder_path = os.path.join(kalimat_base, d)\n",
    "            count += len([f for f in os.listdir(folder_path)])\n",
    "        print(f\"✅ Kalimat Corpus is ready with {count} .txt files\")\n",
    "\n",
    "check_kalimat_structure_os()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292facbf",
   "metadata": {},
   "source": [
    "#### `load_kalimat_articles`\n",
    "We went through each folder `category` pass it to `load_kalimat_articles`. \n",
    "This function will read all the articles, append them to articles list with the following information:\n",
    "- `category`: the name of the folder\n",
    "- `filename`: the name of the file\n",
    "- `text`: the content of the article\n",
    "- `text_length`: the length of the article in characters\n",
    "- `word_count`: the number of words in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0004e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kalimat_articles(category):\n",
    "    category_path = os.path.join(kalimat_base, category)\n",
    "    if not os.path.isdir(category_path):\n",
    "        print(f\"❌ Category '{category}' does not exist in the Kalimat Corpus.\")\n",
    "        return []\n",
    "\n",
    "    articles = []\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(category_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                words = [line.strip() for line in f if line.strip()]\n",
    "                text = \" \".join(words)\n",
    "                articles.append({\n",
    "                    \"category\": category.replace(\"articles\", \"\").upper(),\n",
    "                    \"filename\": filename,\n",
    "                    \"text\": text,\n",
    "                    \"text_length\": len(text),\n",
    "                    \"word_count\": len(words)\n",
    "                })\n",
    "\n",
    "    print(f\"✅ Loaded {len(articles)} articles from category '{category}'\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1168a9",
   "metadata": {},
   "source": [
    "Parellalize the loading of articles using `multiprocessing` to speed up the process.\n",
    "\n",
    "Then in `dataset` we flat the list of articles to one array instead of a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901e61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_articles_parallel():\n",
    "    with mp.Pool(processes=min(len(expected_dirs), int(os.cpu_count() / 2))) as pool:\n",
    "        results = pool.map(load_kalimat_articles, expected_dirs)\n",
    "    \n",
    "    dataset = [article for category_articles in results for article in category_articles]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ac0b8",
   "metadata": {},
   "source": [
    "`if __name__ == \"__main__\"` is Important to solve an issue in Windows OS with `multiprocessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = load_all_articles_parallel()\n",
    "    print(f\"✅ Dataset loaded with {len(dataset)} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4cb2b",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Say Hello to pandas!\n",
    "\n",
    "We will start by constructing a DataFrame from the dataset list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc09f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdataset\u001b[49m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(), df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(), df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c39c5",
   "metadata": {},
   "source": [
    "#### Setup necessary libraries\n",
    "- `nltk` for text processing\n",
    "- `nltk.corpus.stopwords` for stop words\n",
    "- `regex`\n",
    "\n",
    "Then downloading the stop words using `nltk.download()` function.\n",
    "- `stopwords` is a list of common words that are not useful for text analysis (e.g. \"في\", \"من\", \"إلى\", \"على\", \"و\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efafb5bf",
   "metadata": {},
   "source": [
    "Adding additional stopwords based on our analysis of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "stemmer = nltk.stem.ISRIStemmer() # Worsen the results\n",
    "for word in ['في', 'ان', 'ان', 'الى', 'او', 'فى']: arabic_stopwords.add(word)\n",
    "print(f\"Stop words count: {len(arabic_stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee8481",
   "metadata": {},
   "source": [
    "#### `preprocess_text`\n",
    "We applied `preprocess_text` to clean and normalize the Arabic text before modeling.  \n",
    "This function performs the following steps:\n",
    "- **Remove** punctuation, digits (Arabic and English), and English letters.\n",
    "- **Normalize** Arabic letters by unifying variants (e.g., \"أ\", \"إ\", \"آ\" → \"ا\").\n",
    "- **Remove** Arabic diacritics and extra whitespace.\n",
    "- **Tokenize** the text and **remove** Arabic stopwords.\n",
    "- Finally, **join** the tokens back into a cleaned string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    text = re.sub(r'\\p{P}+|\\$', '', text)  # remove all punctuation (English + Arabic)\n",
    "    text = re.sub(r'[0-9٠-٩]', '', text)  # remove Arabic and English digits\n",
    "    text = re.sub(r'[a-zA-Z]', '', text)  # remove English letters\n",
    "    text = re.sub(r'[اآإأ]', 'ا', text)  # replace Arabic letter with hamza with 'ا'\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)  # remove Arabic diacritics\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # clean extra spaces\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in arabic_stopwords]\n",
    "\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocess_text(\"!مرحباً... هذا نَصٌّ تَجْرِيبِيٌ يحتوي على 123 أرقام ٤٥٦، في علامات ترقيم @#$%، كلمات إنجليزية like This.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562dbc4",
   "metadata": {},
   "source": [
    "Now we can apply the `preprocess_text` function to the DataFrame creating a new column `processed_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7641e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ae2b8",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf0802ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from collections import Counter\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "%matplotlib inline\n",
    "\n",
    "fm.fontManager.addfont('arial-unicode-ms.ttf')\n",
    "arabic_font = fm.FontProperties(fname='arial-unicode-ms.ttf')\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = arabic_font.get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_duplicate'] = df.duplicated(subset='processed_text', keep=False)\n",
    "\n",
    "# Group and make sure both True/False appear\n",
    "counts = df.groupby(['category', 'is_duplicate']).size().unstack().reindex(columns=[False, True], fill_value=0)\n",
    "counts.columns = ['Unique', 'Duplicate']\n",
    "print(counts)\n",
    "# Plot\n",
    "counts.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='tab10')\n",
    "plt.title(\"Stacked Bar of Unique vs Duplicate Articles per Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Article Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ffac47",
   "metadata": {},
   "source": [
    "#### 1- Stacked Bar: Unique vs Duplicate Articles per Category\n",
    "While most categories have a healthy distribution of unique articles, the `RELIGION` category has a significant number of duplicate articles. This could bias classification models if not properly handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e4f24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7b021",
   "metadata": {},
   "source": [
    "#### 2- Vocabulary Size\n",
    "After preprocessing, the corpus contains ~235k unique words. Such size will impact the dimensionality of the feature extraction methods like TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "df['processed_text'].str.split().apply(vocab.update)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8a2a3",
   "metadata": {},
   "source": [
    "#### 3- Top 20 Most Common Words (Bar Chart)\n",
    "The most frequent words in the corpus. we can see \"السلطنة\" which make sense as the corpus if of Omani articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6f58b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter()\n",
    "_ = df['processed_text'].str.split().apply(word_counts.update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = word_counts.most_common(20)\n",
    "\n",
    "words, counts = zip(*common_words)\n",
    "display_words = list(map(get_display, map(arabic_reshaper.reshape, words)))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(display_words, counts)\n",
    "plt.title(\"Top 20 Most Common Words in the Corpus\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf74f1c",
   "metadata": {},
   "source": [
    "#### 4- Article Word Counts per Category (Box Plot)\n",
    "The median article length is different across categories. `RELIGION` articles tend to be longer on average, while `SPORTS` articles are shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "df.boxplot(column='word_count', by='category', grid=False, rot=10)\n",
    "plt.title('Article Words Counts per Category') \n",
    "plt.suptitle('')  # Remove the automatic \"Boxplot grouped by\" title\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ef675",
   "metadata": {},
   "source": [
    "#### 5- Top 20 Bigrams (Horizontal Bar Chart)\n",
    "Common bigrams such as \"السلطان قابوس\",\"بن سعيد\", and \"محمد بن\" appear frequently in the corpus, Capturing common word expressions in Omani journalism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2654bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), max_features=20)\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "bigrams = vectorizer.get_feature_names_out()\n",
    "\n",
    "counts = X.sum(axis=0).A1\n",
    "display_words = list(map(get_display, map(arabic_reshaper.reshape, bigrams)))\n",
    "\n",
    "bigrams_counts = list(zip(display_words, counts))\n",
    "bigrams_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "sorted_display_words, sorted_counts = zip(*bigrams_counts)\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(sorted_display_words, sorted_counts)\n",
    "plt.title('Top 20 Bigrams (Sorted)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfcfbcb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Phase 1: Traditional Approaches\n",
    "\n",
    "### Task 1: Traditional Text Classification\n",
    "Traditional machine learning approaches using feature extraction methods and classical algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7599a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7f3f3",
   "metadata": {},
   "source": [
    "We Tried every combination of Traditional methods `BoW` , `TfIdf` and `SVM` , `Naive Bayes` , `Random Forest` for classification.\n",
    "\n",
    "In general `TfIdf` was better than `BoW` as for the algorithms of learning the best accuracy was from `SVM` with `TfIdf` . `Naive bayes` is the fastest with under 1 sec and very good accuracy. `Random Forest` has good accuracy but it took longer time ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "X = tfidf_matrix\n",
    "y = df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "svm_classifier = LinearSVC(random_state=42, C=1.0)\n",
    "nb_classifier = MultinomialNB(alpha=0.01)\n",
    "svm_classifier.fit(X_train, y_train), nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e4e68",
   "metadata": {},
   "source": [
    "#### Results Summary\n",
    "- `SVM` + `TfIdf` : Gave us the best f1-score average accuracy of 92%\n",
    "- Mean Cross Validation Score: 90.60%\n",
    "- `Naive Bayes` + `TfIdf` : Gave us the second best f1-score average accuracy of 88%\n",
    "- Mean Cross Validation Score: 87.73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cd75d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y_true, y_pred, labels=None, normalize=False, figsize=(6, 4), title=\"Confusion Matrix\"):\n",
    "    if labels is None:\n",
    "        labels = sorted(list(set(y_true) | set(y_pred)))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "        fmt = \".2f\"\n",
    "    else:\n",
    "        fmt = \"d\"\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')  # Tilt x-axis labels\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(classifier, X, y):\n",
    "    y_pred = classifier.predict(X)\n",
    "    cv_scores = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "    print(f'\\nCross-Validation Scores: {cv_scores}')\n",
    "    print(f'Mean Cross-Validation Score: {np.mean(cv_scores):.4f}')\n",
    "\n",
    "    print(\"\\n Model Evaluation\")\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    class_report = classification_report(y, y_pred, target_names=sorted(y.unique()))\n",
    "    print(class_report)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = show_confusion_matrix(y_test, y_pred, labels=sorted(y_test.unique()))\n",
    "\n",
    "print(\"SVM Classifier Evaluation\")\n",
    "model_evaluation(svm_classifier, X_test, y_test)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Naive Bayes Classifier Evaluation\")\n",
    "model_evaluation(nb_classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4d1e6",
   "metadata": {},
   "source": [
    "#### Prediction function that uses the SVM + TfIdf model\n",
    "- The funciton takes a string as input\n",
    "- Preprocess the text\n",
    "- Transform the text using the `tfidf_vectorizer`\n",
    "- Predict the category using the `svm_model`\n",
    "- Return the predicted category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c960b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text):\n",
    "    tokenized_text = preprocess_text(text)\n",
    "    X_new = tfidf_vectorizer.transform([tokenized_text])\n",
    "    return svm_classifier.predict(X_new)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33b721",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Task 2: Traditional Text Generation\n",
    "N-gram language models for Arabic text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b33316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CULTURE →\", predict_category(\"أطلقت وزارة الثقافة برنامجًا وطنيًا يهدف إلى إحياء التراث الشعبي من خلال دعم الفنون التقليدية والمهرجانات المحلية التي تسلط الضوء على الهوية السعودية.\"))\n",
    "print(\"ECONOMY →\", predict_category(\"شهدت الأسواق المالية ارتفاعًا ملحوظًا في قيمة الأسهم السعودية بعد إعلان الحكومة عن خطة تنموية جديدة تركز على التنوع الاقتصادي وتقليل الاعتماد على النفط.\"))\n",
    "print(\"INTERNATIONAL →\", predict_category(\"عقدت القمة الخليجية الأوروبية اجتماعها السنوي في بروكسل لمناقشة التحديات العالمية مثل الأمن الغذائي والتغير المناخي وتعزيز التعاون بين الشرق والغرب.\"))\n",
    "print(\"LOCAL →\", predict_category(\"بدأت أمانة المدينة بتنفيذ مشروع توسعة الطرق الداخلية بهدف تخفيف الازدحام المروري، كما تم الإعلان عن إنشاء ممرات مشاة ومواقف ذكية.\"))\n",
    "print(\"RELIGION →\", predict_category(\"حثّ إمام المسجد خلال خطبة الجمعة على التمسك بالقيم الإسلامية ونشر التسامح بين أفراد المجتمع، مشيرًا إلى أهمية الصدق والأمانة في التعاملات اليومية.\"))\n",
    "print(\"SPORTS →\", predict_category(\"تمكن المنتخب الوطني من الفوز على نظيره الإيراني في مباراة مثيرة انتهت بنتيجة ٣-٢، ليضمن التأهل إلى نهائي كأس آسيا وسط فرحة جماهيرية عارمة.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f3052",
   "metadata": {},
   "source": [
    "### 🔍 Observations On The Predictions\n",
    "* The model correctly predicted **5 out of 6** categories.\n",
    "* The **RELIGION** article was misclassified as **LOCAL**, likely due to **data imbalance** or **semantic overlap** in community-related language.\n",
    "* Overall, the model demonstrates **strong accuracy**, with minor limitations in underrepresented categories.\n",
    "\n",
    "Let me know if you want this phrased formally for a report or presentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f7147",
   "metadata": {},
   "source": [
    "### Task 2: Text Generation\n",
    "We needed to use different preprocessing step to get rid of stemming and other steps that will ruin the generation of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_arabic_text(text):\n",
    "    # Remove non-Arabic characters and normalize whitespace\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text) # keep only Arabic characters\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)  # remove Arabic diacritics\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # normalize whitespace\n",
    "    text = re.sub(r'[اآإأ]', 'ا', text)  # replace Arabic letter with hamza with 'ا'\n",
    "\n",
    "    return text\n",
    "preprocess_arabic_text(\"!مرحباً... هذا نَصٌّ تَجْرِيبِيٌ يحتوي على 123 أرقام ٤٥٦، في علامات ترقيم @#$%، كلمات إنجليزية like This.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7c2b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to build n-gram model\n",
    "def build_ngram_model(texts, n):\n",
    "    model = defaultdict(list)\n",
    "    all_words = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Preprocess the text\n",
    "        text = preprocess_arabic_text(text)\n",
    "        # Split into words\n",
    "        words = text.split()\n",
    "        all_words.extend(words)\n",
    "        \n",
    "        # Build n-grams\n",
    "        for i in range(len(words) - n + 1):\n",
    "            # Use tuple of n-1 words as key\n",
    "            prefix = tuple(words[i:i+n-1])\n",
    "            # Use the nth word as value\n",
    "            suffix = words[i+n-1]\n",
    "            model[prefix].append(suffix)\n",
    "    \n",
    "    return model, list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae964451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Function to generate text with a random start word\n",
    "def generate_arabic_text(model, all_words, length=100, n=5):\n",
    "    # Choose a random start word\n",
    "    start_word = random.choice(all_words)\n",
    "    \n",
    "    # Find a valid prefix that contains the start word\n",
    "    valid_prefixes = [prefix for prefix in model.keys() if start_word in prefix]\n",
    "    \n",
    "    # If no valid prefix contains the start word, just use any prefix\n",
    "    if valid_prefixes:\n",
    "        current = random.choice(valid_prefixes)\n",
    "    else:\n",
    "    # Fall back to any random prefix\n",
    "        current = random.choice(list(model.keys()))\n",
    "        start_word = current[0] if len(current) > 0 else start_word  # Update start word to match what we're using\n",
    "\n",
    "    result = list(current)\n",
    "    \n",
    "    # Generate text\n",
    "    for _ in range(length):\n",
    "        if current in model:\n",
    "            # Choose a random next word based on the current n-1 words\n",
    "            next_word = random.choice(model[current])\n",
    "            result.append(next_word)\n",
    "            # Update current context (sliding window)\n",
    "            current = tuple(result[-(n-1):])\n",
    "        else:\n",
    "            # if we reach a dead end, choose a new random prefix\n",
    "            current = random.choice(list(model.keys()))\n",
    "            result.extend(current)\n",
    "    \n",
    "    return start_word, ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eee46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def build_eval_ngram_model(n, texts):\n",
    "    model, all_words = build_ngram_model(texts, n)\n",
    "    print(f\"\\n✅ N-gram model built with n={n} ({len(model)} prefixes)\\n\")\n",
    "\n",
    "    for i in range(3):\n",
    "        start_word, generated_text = generate_arabic_text(model, all_words, length=30, n=n)\n",
    "        print(f\"🔹 Sample {i+1} (start: '{start_word}'):\")\n",
    "        for word in generated_text.split(): \n",
    "            print(word, end=' ', flush=True)\n",
    "            time.sleep(0.05)\n",
    "        print('')  # new line after each sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c880d4",
   "metadata": {},
   "source": [
    "#### Testing the model from n=1 to n=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    build_eval_ngram_model(i+1, df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f4966",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 4. Phase 2: Modern Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cfba5",
   "metadata": {},
   "source": [
    "## Task 1: Deep Learning Text Classification\n",
    "Modern neural network approaches including BiLSTM and Transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "4f2dff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc1ac4d",
   "metadata": {},
   "source": [
    "1- **Tokenization**\n",
    "\n",
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e915406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokenized_text = [text.split() for text in df['processed_text']]\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(word for article in tokenized_text for word in article)\n",
    "\n",
    "# Vocabulary\n",
    "vocab = {word: idx + 2 for idx, (word, count) in enumerate(word_counts.items())}\n",
    "vocab['<PAD>'] = 0  # Padding token\n",
    "vocab['<UNK>'] = 1  # Unknown token\n",
    "\n",
    "# Reverse vocabulary for decoding\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Stats\n",
    "print(f\"Most common words: {word_counts.most_common(10)}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67427b83",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "4de4bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d59ea",
   "metadata": {},
   "source": [
    "1.1 **Tokenizer Encoder and Decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a454d0",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f90eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(article, vocab, max_len):\n",
    "    tokens = [vocab.get(word, vocab['<UNK>']) for word in article]\n",
    "    chunks = []\n",
    "\n",
    "    # Split the tokens into chunks of max_len\n",
    "    for i in range(0, len(tokens), max_len):\n",
    "        chunk = tokens[i:i + max_len]\n",
    "        if len(chunk) < max_len:\n",
    "            chunk += [vocab['<PAD>']] * (max_len - len(chunk))\n",
    "        chunks.append(chunk)\n",
    "        # break # Act as the normal encode function, not chunking\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def decode_text(encoded_article, reverse_vocab):\n",
    "    return ' '.join(reverse_vocab.get(idx, '<UNK>') for idx in encoded_article if idx not in (0, 1))  # Skip PAD and UNK tokens\n",
    "\n",
    "# Test encoding and decoding\n",
    "sample_article = tokenized_text[0]\n",
    "print(f\"Sample article encoded and decoded safely: {' '.join([decode_text(chunk, reverse_vocab) for chunk in encode_text(sample_article, vocab, 100)]) == ' '.join(sample_article)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd6b40",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "a55f27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_transformer(text, max_len=128):\n",
    "    assert max_len < 512, \"Max length for BERT should be less than 512 tokens.\"\n",
    "    \n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_len - 2):\n",
    "        chunk = tokens[i:i + (max_len - 2)]\n",
    "\n",
    "        chunk = [2] + chunk + [3]  # Add [CLS] and [SEP] tokens\n",
    "\n",
    "        padding_length = max_len - len(chunk)\n",
    "        chunk += [0] * padding_length  # Pad with zeros\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def decode_text_transformer(encoded_article):\n",
    "    decoded = tokenizer.decode(encoded_article, skip_special_tokens=True)\n",
    "    return decoded.replace('  ', ' ').strip()  # Clean up double spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم\", max_length=24, truncation=True, padding='max_length'))\n",
    "print(encode_text_transformer(\"السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم\", max_len=12))\n",
    "print(decode_text_transformer(encode_text_transformer(\"السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم\")[0]) == \"السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم السلام عليكم\")\n",
    "# There will be less padding in the encode_text_transformer because chunking adds CLS and SEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b95201f",
   "metadata": {},
   "source": [
    "1.2 **Label Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = {idx: label for idx, label in enumerate(label_encoder.classes_)}\n",
    "label2id = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
    "decode_labels = lambda idx: label_encoder.inverse_transform(idx)\n",
    "print(f\"Label mapping: {label_mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ad6db",
   "metadata": {},
   "source": [
    "2- **Dataset Preparation**\n",
    "\n",
    "2.1- **Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "219b53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        self.lengths = (self.texts != vocab['<PAD>']).sum(dim=1)  # Calculate lengths for each text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.lengths[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b35f5",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "1d19f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTextDataset(Dataset):\n",
    "    def __init__(self, tokenized_text, labels, tokenizer, max_len, split = 'train'):\n",
    "        self.tokenized_text = torch.tensor(tokenized_text, dtype=torch.long)\n",
    "        self.attention_mask = self.tokenized_text != 0\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenized_text[idx]\n",
    "        # if self.split == 'test':\n",
    "        #     input_ids = decode_text_transformer(input_ids)\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d279b7e",
   "metadata": {},
   "source": [
    "3- **Model Architecture**\n",
    "\n",
    "3.1- **LSTM Model**: Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "9abf4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, pad_idx):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, num_layers=max(2, num_layers//2), dropout=0.4, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # normalization layer\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "        # fc layer\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "\n",
    "        # Second BiLSTM layer\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=max(2, num_layers//2), dropout=0.4, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # normalization layer\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Pack the sequence for LSTM\n",
    "        packed1 = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out1, _ = self.lstm1(packed1)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        lstm1_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out1, batch_first=True)\n",
    "\n",
    "        lstm1_out = self.norm1(lstm1_out) # (batch_size, seq_len, hidden_dim * 2)\n",
    "        fc1_out = self.fc1(lstm1_out) # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        # Pack the sequence for the second LSTM\n",
    "        packed2 = nn.utils.rnn.pack_padded_sequence(\n",
    "            fc1_out, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out2, (hidden, _) = self.lstm2(packed2)\n",
    "        lstm2_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out2, batch_first=True)\n",
    "        \n",
    "        lstm2_out = self.norm2(lstm2_out + lstm1_out) # Residual connection\n",
    "\n",
    "        # Use the final forward and backward hidden states\n",
    "        out = torch.cat((hidden[-2], hidden[-1]), dim=1)  # (batch_size, hidden_dim * 2)\n",
    "        out = self.dropout2(out)\n",
    "        return self.fc2(out)  # (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc6980",
   "metadata": {},
   "source": [
    "3.2- **Transformer Based Model**: AraBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "3e44e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# The model can be used like this:\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_mapping), id2label=label_mapping, label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d1fd1",
   "metadata": {},
   "source": [
    "4- **Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "86c64c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde72b3",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "97df8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 100  # Evaluate every 100 batches\n",
    "eval_iters = 10 # Number of iterations for evaluation\n",
    "\n",
    "max_len = 500  # Maximum length of sequences\n",
    "batch_size = 256 # Batch size for training\n",
    "bilstm_num_epochs = 30  # Number of epochs for BiLSTM training\n",
    "lr = 2e-3  # Learning rate\n",
    "\n",
    "embedding_dim = 300  # Dimension of word embeddings\n",
    "num_layers = 6  # Number of LSTM layers\n",
    "hidden_dim = 256  # Hidden dimension for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e38aaf",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "25bcc28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './results'\n",
    "evaluation_strategy = 'epoch' # # Evaluate at the end of each epoch\n",
    "save_strategy = 'epoch'  # Save model at the end of each epoch\n",
    "tf_learning_rate = 4e-5 # 1e-4\n",
    "per_device_train_batch_size = 256\n",
    "per_device_eval_batch_size = 32\n",
    "gradient_accumulation_steps = 2\n",
    "num_train_epochs = 8\n",
    "weight_decay = 0.01\n",
    "logging_dir = \"./logs\"\n",
    "load_best_model_at_end = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c92934",
   "metadata": {},
   "source": [
    "4.1- **Data Loaders**\n",
    "\n",
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1712cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_texts_BiLSTM = []\n",
    "chunked_labels_BiLSTM = []\n",
    "\n",
    "\n",
    "for article, label in zip(tokenized_text, encoded_labels):\n",
    "    chunksBiLSTM = encode_text(article, vocab, max_len)\n",
    "\n",
    "    chunked_texts_BiLSTM.extend(chunksBiLSTM)\n",
    "    chunked_labels_BiLSTM.extend([label] * len(chunksBiLSTM))\n",
    "\n",
    "X_train, X_devtest, y_train, y_devtest = train_test_split(\n",
    "    chunked_texts_BiLSTM, chunked_labels_BiLSTM, test_size=0.2, stratify=chunked_labels_BiLSTM, random_state=42\n",
    ")\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_devtest, y_devtest, test_size=0.5, stratify=y_devtest, random_state=42\n",
    ")\n",
    "\n",
    "biLSTMtrain_dataset = BiLSTMTextDataset(X_train, y_train)\n",
    "biLSTMdev_dataset = BiLSTMTextDataset(X_dev, y_dev)\n",
    "biLSTMtest_dataset = BiLSTMTextDataset(X_test, y_test)\n",
    "\n",
    "bitrain_loader = DataLoader(biLSTMtrain_dataset, batch_size=batch_size, shuffle=True)\n",
    "bidev_loader = DataLoader(biLSTMdev_dataset, batch_size=batch_size, shuffle=False)\n",
    "bitest_loader = DataLoader(biLSTMtest_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# stats\n",
    "print(f\"BiLSTM Train dataset size: {len(biLSTMtrain_dataset)}\")\n",
    "print(f\"BiLSTM Dev dataset size: {len(biLSTMdev_dataset)}\")\n",
    "print(f\"BiLSTM Test dataset size: {len(biLSTMtest_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2de3c",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabert preprocessing\n",
    "df['arabert_text'] = df['text'].apply(arabert_prep.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_texts_Transformer = []\n",
    "chunked_labels_Transformer = []\n",
    "\n",
    "for article, label in zip(df['arabert_text'].tolist(), encoded_labels):\n",
    "    chucksTransformer = encode_text_transformer(article, max_len=max_len)\n",
    "\n",
    "    chunked_texts_Transformer.extend(chucksTransformer)\n",
    "    chunked_labels_Transformer.extend([label] * len(chucksTransformer))\n",
    "\n",
    "X_train, X_devtest, y_train, y_devtest = train_test_split(\n",
    "    chunked_texts_Transformer, chunked_labels_Transformer, test_size=0.2, stratify=chunked_labels_Transformer, random_state=42\n",
    ")\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_devtest, y_devtest, test_size=0.5, stratify=y_devtest, random_state=42\n",
    ")\n",
    "\n",
    "tFtrain_dataset = TransformerTextDataset(X_train, y_train, tokenizer, max_len)\n",
    "tFdev_dataset = TransformerTextDataset(X_dev, y_dev, tokenizer, max_len)\n",
    "tFtest_dataset = TransformerTextDataset(X_test, y_test, tokenizer, max_len, split='test')\n",
    "\n",
    "tFtrain_loader = DataLoader(tFtrain_dataset, batch_size=per_device_train_batch_size, shuffle=True)\n",
    "tFdev_loader = DataLoader(tFdev_dataset, batch_size=per_device_train_batch_size, shuffle=False)\n",
    "tFtest_loader = DataLoader(tFtest_dataset, batch_size=per_device_eval_batch_size, shuffle=False)\n",
    "\n",
    "# stats\n",
    "print(f\"Transformer Train dataset size: {len(tFtrain_dataset)}\")\n",
    "print(f\"Transformer Dev dataset size: {len(tFdev_dataset)}\")\n",
    "print(f\"Transformer Test dataset size: {len(tFtest_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16bc627",
   "metadata": {},
   "source": [
    "4.1- **Evaluation Function**: To evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "2a3ee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, transformer=False):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader: # x: (batch_size, seq_len), y: (batch_size,)\n",
    "\n",
    "            if transformer:\n",
    "                print(f\"{cnt}/{len(test_loader)}\", end='\\r')\n",
    "                x = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                y = batch['labels'].to(device)\n",
    "                outputs = model(x, attention_mask=attention_mask)  # outputs: (batch_size, num_classes)\n",
    "                outputs = outputs.logits\n",
    "            else:\n",
    "                x = batch[0].to(device)\n",
    "                lengths = batch[1].to(device)\n",
    "                y = batch[2].to(device)\n",
    "                outputs = model(x, lengths)  # outputs: (batch_size, num_classes)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            cnt += 1\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d3caf",
   "metadata": {},
   "source": [
    "4.2- **Model Training**: train_model function to train given a model, loader, criterion, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "9e7e7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    step = 0\n",
    "\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, lengths, y in train_loader: # (x: (batch_size, seq_len), y: (batch_size,))\n",
    "        x, lengths, y = x.to(device), lengths.to(device), y.to(device)\n",
    "        # Forward pass:\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(x, lengths)\n",
    "            loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update running loss & accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        acc = (preds == y).float().mean().item()\n",
    "        running_acc = running_acc * 0.90 + acc * 0.10 if running_acc > 0 else acc\n",
    "        running_loss = running_loss * 0.90 + loss.item() * 0.10 if running_loss > 0 else loss.item()\n",
    "        step += 1\n",
    "\n",
    "        print(f\"step {step:4d} | loss: {running_loss:.6f} | acc: {running_acc:.6f}\", end='\\r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15beecd",
   "metadata": {},
   "source": [
    "4.3- **Training Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971d8e5",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6bb5d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import math\n",
    "\n",
    "# Learning rate scheduler\n",
    "\n",
    "warmup_steps = 60\n",
    "total_steps = len(bitrain_loader) * bilstm_num_epochs\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.01 + 0.99 * 0.5 * (1.0 + math.cos(math.pi * progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "81f3db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBiLSTM = BiLSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,  # Embedding dimension\n",
    "    hidden_dim=hidden_dim,  # Hidden dimension for LSTM\n",
    "    num_layers=num_layers,  # Number of LSTM layers\n",
    "    num_classes=len(label_mapping),  # Number of classes\n",
    "    pad_idx=vocab['<PAD>']  # Padding index\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(modelBiLSTM.parameters(), lr=lr, weight_decay=1e-5)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087192ca",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTF = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_mapping),\n",
    "    id2label=label_mapping,\n",
    "    label2id=label2id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead26e5b",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "d1d20e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use high precision for float32 matrix multiplication to improve performance\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "modelBiLSTM = torch.compile(modelBiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ff271",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(bilstm_num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    train_model(modelBiLSTM, bitrain_loader, criterion, optimizer, scheduler, device)\n",
    "    y_pred, y_true = evaluate_model(modelBiLSTM, bidev_loader, device)\n",
    "    acc = (np.array(y_pred) == np.array(y_true)).mean()\n",
    "    print(f\"validation accuracy: {acc:.4f}, f1: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Updated learning rate: {param_group['lr']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "ed7c440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(modelBiLSTM.state_dict(), \"bilstm_best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ec1c7",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c845dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=evaluation_strategy,\n",
    "    save_strategy=save_strategy,\n",
    "    learning_rate=tf_learning_rate,\n",
    "    bf16=True,  # instead of fp16\n",
    "    fp16=False,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir=logging_dir,\n",
    "    load_best_model_at_end=load_best_model_at_end\n",
    "    # torch_compile=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=modelTF,\n",
    "    args=training_args,\n",
    "    train_dataset=tFtrain_dataset,\n",
    "    eval_dataset=tFdev_dataset,\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': (np.argmax(p.predictions, axis=1) == p.label_ids).mean(),\n",
    "        'f1': f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='weighted')\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "del modelBiLSTM\n",
    "del modelTF\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fccd05b",
   "metadata": {},
   "source": [
    "5- **Evaluation And Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b22c3",
   "metadata": {},
   "source": [
    "- For the BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Evaluation (On Test Set):\")\n",
    "y_pred, y_true = evaluate_model(modelBiLSTM, bitest_loader, device)\n",
    "acc = (np.array(y_pred) == np.array(y_true)).mean()\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "show_confusion_matrix(decode_labels(y_true), decode_labels(y_pred), labels=sorted(set(decode_labels(y_true)) | set(decode_labels(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_predict(text, model, vocab, max_len=500):\n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_text(text)\n",
    "    tokens = processed_text.split()\n",
    "    \n",
    "    # Encode the text\n",
    "    encoded = [vocab.get(word, vocab['<UNK>']) for word in tokens]\n",
    "    \n",
    "    # Pad or truncate to max_len\n",
    "    if len(encoded) < max_len:\n",
    "        encoded += [vocab['<PAD>']] * (max_len - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:max_len]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor([encoded], dtype=torch.long).to(device)\n",
    "    length_tensor = torch.tensor([len(tokens)], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor, length_tensor)\n",
    "        prediction = torch.argmax(outputs, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    return label_encoder.inverse_transform([prediction])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ec2cd",
   "metadata": {},
   "source": [
    "- For the Tranformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "14ae1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "checkpoint_path = \"results/checkpoint-288-best\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdbe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel Evaluation (On Test Set):\")\n",
    "y_pred, y_true = evaluate_model(model, tFtest_loader, device, transformer=True)\n",
    "acc = (np.array(y_pred) == np.array(y_true)).mean()\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "class_report = classification_report(y_true, y_pred, target_names=label_encoder.classes_)\n",
    "print(class_report)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "show_confusion_matrix(decode_labels(y_true), decode_labels(y_pred), labels=sorted(set(decode_labels(y_true)) | set(decode_labels(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabert_predict(text, model, tokenizer, max_len=500):\n",
    "    # Preprocess the text using AraBERT preprocessing\n",
    "    processed_text = arabert_prep.preprocess(text)\n",
    "    \n",
    "    # Tokenize and encode\n",
    "    tokens = tokenizer.encode(processed_text, add_special_tokens=True, \n",
    "                             max_length=max_len, truncation=True, padding='max_length')\n",
    "    attention_mask = [1 if token != 0 else 0 for token in tokens]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "    attention_mask_tensor = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask_tensor)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    return label_encoder.inverse_transform([prediction])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b953d12",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 5. Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f1481",
   "metadata": {},
   "source": [
    "### 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1106138",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 7. Bonus: Interactive Demo\n",
    "\n",
    "### Interactive Text Classification Demo\n",
    "Here we demonstrate all our trained models by testing them on sample Arabic texts from different categories. Each model uses its own preprocessing and prediction pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92a559",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Model Comparison on Sample Texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test samples for each category\n",
    "test_samples = [\n",
    "    (\"أطلقت وزارة الثقافة برنامجًا وطنيًا يهدف إلى إحياء التراث الشعبي من خلال دعم الفنون التقليدية والمهرجانات المحلية التي تسلط الضوء على الهوية السعودية.\", \"CULTURE\"),\n",
    "    (\"شهدت الأسواق المالية ارتفاعًا ملحوظًا في قيمة الأسهم السعودية بعد إعلان الحكومة عن خطة تنموية جديدة تركز على التنوع الاقتصادي وتقليل الاعتماد على النفط.\", \"ECONOMY\"),\n",
    "    (\"عقدت القمة الخليجية الأوروبية اجتماعها السنوي في بروكسل لمناقشة التحديات العالمية مثل الأمن الغذائي والتغير المناخي وتعزيز التعاون بين الشرق والغرب.\", \"INTERNATIONAL\"),\n",
    "    (\"بدأت أمانة المدينة بتنفيذ مشروع توسعة الطرق الداخلية بهدف تخفيف الازدحام المروري، كما تم الإعلان عن إنشاء ممرات مشاة ومواقف ذكية.\", \"LOCAL\"),\n",
    "    (\"حثّ إمام المسجد خلال خطبة الجمعة على التمسك بالقيم الإسلامية ونشر التسامح بين أفراد المجتمع، مشيرًا إلى أهمية الصدق والأمانة في التعاملات اليومية.\", \"RELIGION\"),\n",
    "    (\"تمكن المنتخب الوطني من الفوز على نظيره الإيراني في مباراة مثيرة انتهت بنتيجة ٣-٢، ليضمن التأهل إلى نهائي كأس آسيا وسط فرحة جماهيرية عارمة.\", \"SPORTS\")\n",
    "]\n",
    "\n",
    "print(\"🔍 INTERACTIVE MODEL COMPARISON DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (text, expected) in enumerate(test_samples, 1):\n",
    "    print(f\"\\n📝 Sample {i} (Expected: {expected}):\")\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    print(\"\\n🔮 Predictions:\")\n",
    "    \n",
    "    # Traditional method prediction\n",
    "    traditional_pred = predict_category(text)\n",
    "    print(f\"   Traditional (SVM): {traditional_pred} {'✅' if traditional_pred == expected else '❌'}\")\n",
    "    \n",
    "    # BiLSTM prediction\n",
    "    bilstm_pred = lstm_predict(text, modelBiLSTM, vocab)\n",
    "    print(f\"   BiLSTM: {bilstm_pred} {'✅' if bilstm_pred == expected else '❌'}\")\n",
    "    \n",
    "    # AraBERT prediction  \n",
    "    arabert_pred = arabert_predict(text, model, tokenizer)\n",
    "    print(f\"   AraBERT: {arabert_pred} {'✅' if arabert_pred == expected else '❌'}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afc775",
   "metadata": {},
   "source": [
    "### Task 2: Arabic Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8fefd9",
   "metadata": {},
   "source": [
    "### 2.2 Traditional Approach: Seq2Seq with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c76c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess Arabic summarization dataset\n",
    "df_sum = pd.read_excel(\"Text summarization dataset.xlsx\")\n",
    "df_sum = df_sum.iloc[1:].reset_index(drop=True)\n",
    "df_sum.columns = ['summary', 'text']\n",
    "df_sum = df_sum.dropna(subset=['summary', 'text'])\n",
    "df_sum.head()\n",
    "df_sum.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c49a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)\n",
    "    text = re.sub(r'ى', 'ي', text)\n",
    "    text = re.sub(r'ؤ', 'و', text)\n",
    "    text = re.sub(r'ئ', 'ي', text)\n",
    "    text = re.sub(r'ة', 'ه', text)\n",
    "    text = re.sub(r'[\\u064B-\\u0652]', '', text)  # remove Arabic diacritics\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply normalization\n",
    "df_sum['text'] = df_sum['text'].apply(normalize_arabic)\n",
    "df_sum['summary'] = df_sum['summary'].apply(normalize_arabic)\n",
    "print(f\"After preprocessing: {df_sum.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52df6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Prepare sequences\n",
    "def prepare_sequences(df_sum, max_text_len=128, max_summary_len=32):\n",
    "    texts = df_sum['text']\n",
    "    summaries = df_sum['summary']\n",
    "    all_texts = list(texts) + list(summaries)\n",
    "    tokenizer = Tokenizer(filters='', oov_token=None)\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    \n",
    "    # Add special tokens\n",
    "    tokenizer.word_index['<sos>'] = len(tokenizer.word_index) + 1\n",
    "    tokenizer.word_index['<eos>'] = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    summary_sequences = tokenizer.texts_to_sequences(summaries)\n",
    "    \n",
    "    # Add <sos> and <eos> to summaries\n",
    "    summary_sequences = [[tokenizer.word_index['<sos>']] + seq + [tokenizer.word_index['<eos>']] \n",
    "                        for seq in summary_sequences]\n",
    "    \n",
    "    # Pad sequences\n",
    "    text_padded = pad_sequences(text_sequences, maxlen=max_text_len, padding='post')\n",
    "    summary_padded = pad_sequences(summary_sequences, maxlen=max_summary_len, padding='post')\n",
    "    \n",
    "    return tokenizer, text_padded, summary_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, texts, summaries):\n",
    "        self.texts = texts          # خله list او numpy\n",
    "        self.summaries = summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "            torch.tensor(self.summaries[idx], dtype=torch.long),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef654d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Traditional Seq2Seq Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, x, hidden, cell):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, hidden_dim)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, hidden_dim)\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(device)\n",
    "        _, (hidden, cell) = self.encoder(src)\n",
    "        input = trg[:, 0].unsqueeze(1)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(2)\n",
    "            input = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76a241",
   "metadata": {},
   "source": [
    "Evaluation funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ec589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_seq2seq_rouge(model, loader, tokenizer, device, max_len=128):\n",
    "    model.eval()\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "    totals = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "    n = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in loader:                       # src, trg: (B, T)\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            # توليد الملخص (من دون teacher forcing)\n",
    "            out = model(src, trg[:, :1], teacher_forcing_ratio=0.)   # (B, T, V)\n",
    "            gen_ids = out.argmax(-1)                                # (B, T)\n",
    "\n",
    "            # IDs → نص\n",
    "            preds = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "            refs  = tokenizer.batch_decode(trg,    skip_special_tokens=True)\n",
    "\n",
    "            # ROUGE لكل مثال\n",
    "            for p, r in zip(preds, refs):\n",
    "                s = scorer.score(r, p)\n",
    "                for k in totals: totals[k] += s[k].fmeasure\n",
    "                n += 1\n",
    "\n",
    "    return {k: v / n for k, v in totals.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77d1e2",
   "metadata": {},
   "source": [
    "training lop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051b789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq_model(model, train_loader, optimizer, criterion, scaler, epoch):\n",
    "    model.train()\n",
    "    step = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for src, trg in train_loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(src, trg)\n",
    "            # Reshape output and target for loss calculation\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss = running_loss * 0.90 + loss.item() * 0.10 if running_loss > 0 else loss.item()\n",
    "        step += 1\n",
    "\n",
    "        print(f\"step {step:4d} | loss: {running_loss:.6f}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04823c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "seq2seq_max_text_len = 128\n",
    "seq2seq_max_summary_len = 32\n",
    "seq2seq_batch_size = 128\n",
    "seq2seq_epochs = 10\n",
    "seq2seq_learning_rate = 1e-4\n",
    "seq2seq_embedding_dim = 256\n",
    "seq2seq_hidden_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78364c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, text_padded, summary_padded = prepare_sequences(df_sum, max_text_len=seq2seq_max_text_len, max_summary_len=seq2seq_max_summary_len)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17de8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_devtest, y_train, y_devtest = train_test_split(\n",
    "    text_padded, summary_padded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_devtest, y_devtest, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "seq2seq_train_dataset = SummarizationDataset(X_train, y_train)\n",
    "seq2seq_dev_dataset = SummarizationDataset(X_dev, y_dev)\n",
    "seq2seq_test_dataset = SummarizationDataset(X_test, y_test)\n",
    "\n",
    "seq2seq_train_loader = DataLoader(seq2seq_train_dataset, batch_size=seq2seq_batch_size, shuffle=False)\n",
    "seq2seq_dev_loader = DataLoader(seq2seq_dev_dataset, batch_size=seq2seq_batch_size, shuffle=False)\n",
    "seq2seq_test_loader = DataLoader(seq2seq_test_dataset, batch_size=seq2seq_batch_size, shuffle=False)\n",
    "\n",
    "# stats\n",
    "print(f\"Seq2Seq Train dataset size: {len(seq2seq_train_dataset)}\")\n",
    "print(f\"Seq2Seq Dev dataset size: {len(seq2seq_dev_dataset)}\")\n",
    "print(f\"Seq2Seq Test dataset size: {len(seq2seq_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler\n",
    "\n",
    "modelSeq2Seq = Seq2Seq(\n",
    "    vocab_size=vocab_size, \n",
    "    embedding_dim=seq2seq_embedding_dim, \n",
    "    hidden_dim=seq2seq_hidden_dim\n",
    "    ).to(device)\n",
    "\n",
    "# Training setup\n",
    "seq2seq_criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "seq2seq_optimizer = optim.Adam(modelSeq2Seq.parameters(), lr=seq2seq_learning_rate, weight_decay=1e-5)\n",
    "seq2seq_scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(seq2seq_epochs):\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    train_seq2seq_model(modelSeq2Seq, seq2seq_train_loader, seq2seq_optimizer, seq2seq_criterion, seq2seq_scaler, epoch)\n",
    "    rouge = evaluate_seq2seq_rouge(modelSeq2Seq, seq2seq_dev_loader, tokenizer, device)\n",
    "    print(f\"ROUGE-1: {rouge['rouge1']:.4f} | \"\n",
    "          f\"ROUGE-2: {rouge['rouge2']:.4f} | \"\n",
    "          f\"ROUGE-L: {rouge['rougeL']:.4f}\")\n",
    "torch.save(modelSeq2Seq.state_dict(), \"arabic_summarizer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc23028",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2.2 Modern Approach: Transformer-Based (AraBART)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09ba52",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation of both approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cffa6ed",
   "metadata": {},
   "source": [
    "Loading the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSeq2Seq.load_state_dict(torch.load(\"arabic_summarizer.pth\"))\n",
    "# modelTfSeq2Seq = AutoModel ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the traditional approach\n",
    "rouge = evaluate_seq2seq_rouge(modelSeq2Seq, seq2seq_dev_loader, tokenizer, device)\n",
    "print(f\"ROUGE-1: {rouge['rouge1']:.4f} | \"\n",
    "      f\"ROUGE-2: {rouge['rouge2']:.4f} | \"\n",
    "      f\"ROUGE-L: {rouge['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db769f81",
   "metadata": {},
   "source": [
    "### 2.4 Summarization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136afa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "تعتبر التغذية السليمة أساس الصحة الجيدة. يجب أن يحتوي النظام الغذائي اليومي على مجموعة متنوعة من الأطعمة المغذية. الخضروات والفواكه الطازجة توفر الفيتامينات والمعادن الضرورية للجسم. البروتينات الموجودة في اللحوم والأسماك والبقوليات تساعد في بناء العضلات وإصلاح الأنسجة.\n",
    "\n",
    "من المهم تناول وجبات منتظمة وتجنب الوجبات السريعة الغنية بالدهون والسكريات. شرب الماء بكميات كافية يساعد في الحفاظ على رطوبة الجسم وتحسين عملية الهضم. يجب أيضاً التقليل من المشروبات الغازية والعصائر المحلاة.\n",
    "\n",
    "تناول وجبة الإفطار يعتبر من أهم العادات الصحية. فهي تمد الجسم بالطاقة اللازمة لبدء اليوم بنشاط. من المهم أيضاً تناول وجبات خفيفة صحية بين الوجبات الرئيسية للحفاظ على مستوى الطاقة في الجسم.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, text, max_length=128):\n",
    "    model.eval()\n",
    "    text = normalize_arabic(text)\n",
    "    text_seq = tokenizer.texts_to_sequences([text])[0]\n",
    "    text_padded = pad_sequences([text_seq], maxlen=seq2seq_max_text_len, padding='post')\n",
    "    text_tensor = torch.tensor(text_padded, dtype=torch.long).to(device)\n",
    "    sos_idx = tokenizer.word_index.get('<sos>', 2)\n",
    "    eos_idx = tokenizer.word_index.get('<eos>', 3)\n",
    "    decoder_input = torch.tensor([[sos_idx]], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        _, (hidden, cell) = model.encoder(text_tensor)\n",
    "    summary = []\n",
    "    for _ in range(max_length):\n",
    "        output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
    "        predicted = output.argmax(2)\n",
    "        pred_idx = predicted.item()\n",
    "        if pred_idx == eos_idx:\n",
    "            break\n",
    "        summary.append(pred_idx)\n",
    "        decoder_input = predicted\n",
    "    idx2word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    summary_words = [idx2word.get(idx, '') for idx in summary]\n",
    "    return ' '.join(summary_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980e530",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Instructions for Deployment and Usage\n",
    "\n",
    "**To use the trained models independently:**\n",
    "\n",
    "1. **Load the saved models:**\n",
    "   ```python\n",
    "   # For traditional models  \n",
    "   svm_model = joblib.load('svm_model.pkl')\n",
    "   tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "   \n",
    "   # For BiLSTM\n",
    "   bilstm_model.load_state_dict(torch.load('bilstm_best_model.pth'))\n",
    "   \n",
    "   # For AraBERT\n",
    "   arabert_model = AutoModelForSequenceClassification.from_pretrained('results/checkpoint-288-best')\n",
    "   ```\n",
    "\n",
    "2. **Use the prediction functions:**\n",
    "   ```python\n",
    "   # Traditional\n",
    "   result = predict_category(\"نص عربي\")\n",
    "   \n",
    "   # BiLSTM  \n",
    "   result = lstm_predict(\"نص عربي\", modelBiLSTM, vocab)\n",
    "   \n",
    "   # AraBERT\n",
    "   result = arabert_predict(\"نص عربي\", model, tokenizer)\n",
    "   ```\n",
    "\n",
    "3. **For web deployment:** Create a Flask/FastAPI wrapper around these functions\n",
    "\n",
    "**System Requirements:**\n",
    "- Python 3.8+, PyTorch 1.12+, Transformers 4.21+, scikit-learn 1.1+, NLTK with Arabic stopwords\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
